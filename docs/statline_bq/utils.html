<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.8.5" />
<title>statline_bq.utils API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js" integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
<style>
dd p {
white-space: pre-wrap;
}
</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>statline_bq.utils</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import subprocess
from typing import Union, Iterable
from os import remove, listdir, rmdir, PathLike
from pathlib import Path
from glob import glob
import requests
import json
import dask.bag as db
from datetime import datetime
from pyarrow import json as pa_json
import pyarrow.parquet as pq
from google.cloud import storage
from google.cloud import bigquery
from statline_bq.config import Config, Gcp
from google.api_core import exceptions


def check_v4(id: str, third_party: bool = False) -&gt; str:
    &#34;&#34;&#34;Checks whether a certain CBS table exists as OData Version &#34;v4&#34;.

    Parameters
    ----------
    id: str
        CBS Dataset id, i.e. &#34;83583NED&#34;

    third_party: bool, default=False
        Flag to indicate dataset is not originally from CBS. Set to true
        to use dataderden.cbs.nl as base url (not available in v4 yet).

    Returns
    -------
    odata_version: str
        &#34;v4&#34; if exists as odata v4, &#34;v3&#34; otherwise.
    &#34;&#34;&#34;

    base_url = {
        True: None,  # currently no IV3 links in ODATA V4,
        False: f&#34;https://odata4.cbs.nl/CBS/{id}&#34;,
    }
    r = requests.get(base_url[third_party])
    if (
        r.status_code == 200
    ):  # TODO: Is this the best check to use? Maybe if not 404? Or something else?
        odata_version = &#34;v4&#34;
    else:
        odata_version = &#34;v3&#34;
    return odata_version


def create_dir(path: Path) -&gt; Path:
    &#34;&#34;&#34;Checks whether a path exists and is a directory, and creates it if not.

    Parameters
    ----------
    path: Path
        A path to the directory.

    Returns
    -------
    path: Path
        The same input path, to new or existing directory.
    &#34;&#34;&#34;

    try:
        path = Path(path)
        if not (path.exists() and path.is_dir()):
            path.mkdir(parents=True)
        return path
    except TypeError as error:
        print(f&#34;Error trying to find {path}: {error!s}&#34;)
        return None


def get_dataset_description(urls: dict, odata_version: str) -&gt; str:
    &#34;&#34;&#34;Gets a CBS dataset description text.
    
    Wrapper function to call the correct version function which in turn gets
    the dataset description according to the odata version: &#34;v3&#34; or &#34;v4&#34;.

    Parameters
    ----------
    urls: dict
        Dictionary holding urls of the dataset from CBS.
        NOTE: urls[&#34;Properties&#34;] (for v4) or urls[&#34;TableInfos&#34;] (for v3)
        must be present in order to access the dataset description.

    odata_version: str
        version of the odata for this dataset - must be either &#34;v3&#34; or &#34;v4&#34;.

    Returns
    -------
    description: str
        The description of the dataset from CBS.

    Examples
    --------
    &gt;&gt;&gt; from statline_bq.utils import get_dataset_description
    &gt;&gt;&gt; urls = {
    ...         &#34;TableInfos&#34;: &#34;https://opendata.cbs.nl/ODataFeed/odata/83583NED/TableInfos&#34;,  
    ...         &#34;UntypedDataSet&#34;: &#34;https://opendata.cbs.nl/ODataFeed/odata/83583NED/UntypedDataSet&#34;
    ...         }
    &gt;&gt;&gt; odata_version = &#34;v3&#34;
    &gt;&gt;&gt; description_text = get_dataset_description(urls, odata_version=odata_version)
    &gt;&gt;&gt; description_text
    #Text describing the dataset will print
    &#34;&#34;&#34;

    if odata_version.lower() == &#34;v4&#34;:
        description = get_dataset_description_v4(urls[&#34;Properties&#34;])
    elif odata_version.lower() == &#34;v3&#34;:
        description = get_dataset_description_v3(urls[&#34;TableInfos&#34;])
    else:
        raise ValueError(&#34;odata version must be either &#39;v3&#39; or &#39;v4&#39;&#34;)
    return description


def get_dataset_description_v3(url_table_infos: str) -&gt; str:
    &#34;&#34;&#34;Gets the description of a v3 odata dataset from CBS provided in url_table_infos.

    Usually wrapped in `get_dataset_description()`, and it is better practice
    to call it wrapped to allow for both &#34;v3&#34; and &#34;v4&#34; functionality.

    Parameters
    ----------
    url_table_infos: str
        The url for a dataset&#39;s &#34;TableInfos&#34; table as string.

    Returns
    -------
    description: str
        A string with the dataset&#39;s description
    &#34;&#34;&#34;

    # Get JSON format of data set.
    url_table_infos = &#34;?&#34;.join((url_table_infos, &#34;$format=json&#34;))

    data_info = requests.get(url_table_infos).json()  # Is of type dict()

    data_info_values = data_info[&#34;value&#34;]  # Is of type list

    # Get short description as text
    description = data_info_values[0][&#34;ShortDescription&#34;]

    return description


def get_dataset_description_v4(url_table_properties: str) -&gt; str:
    &#34;&#34;&#34;Gets table description of a table in CBS odata V4.

    Usually wrapped in `get_dataset_description()`, and it is better practice
    to call it wrapped to allow for both &#34;v3&#34; and &#34;v4&#34; functionality.

    Parameters
    ----------
    url_table_properties: str
        The url for a dataset&#39;s &#34;Properties&#34; table as string.

    Returns
    -------
    description: str
        A string with the dataset&#39;s description
    &#34;&#34;&#34;

    r = requests.get(url_table_properties).json()
    return r[&#34;Description&#34;]


def write_description_to_file(
    id: str,
    description_text: str,
    pq_dir: Union[Path, str],
    source: str = &#34;cbs&#34;,
    odata_version: str = None,
) -&gt; Path:
    &#34;&#34;&#34;Writes a string into a text file at a given location.

    Writes a dataset description string into a txt file and places that
    file in a directory alongside the rest of that dataset&#39;s tables (assuming
    it, and they exist). The file is named according to the same conventions
    used for the tables, and placed in the directory accordingly, namely:

        &#34;{source}.{odata_version}.{id}_Description.txt&#34;

    for example:

        &#34;cbs.v3.83583NED_Description.txt&#34;

    Parameters
    ----------
    id: str
        CBS Dataset id, i.e. &#34;83583NED&#34;
    description_text: str
        The dataset description text to be written into the file.
    pq_dir: Path or str
        Path to directory where the file will be stored.
    source: str, default=&#34;cbs&#34;
        The source of the dataset. Currently only &#34;cbs&#34; is relevant.
    odata_version: str
        The version of the OData for this dataset - should be &#34;v3&#34; or &#34;v4&#34;.

    Returns
    -------
    description_file: Path
        A path of the output txt file
    &#34;&#34;&#34;

    description_file = Path(pq_dir) / Path(
        f&#34;{source}.{odata_version}.{id}_Description.txt&#34;
    )
    with open(description_file, &#34;w+&#34;) as f:
        f.write(description_text)
    return description_file


# Currently not implemented. Possibly not needed.
def get_odata_v4_curl(  # TODO -&gt; CURL command does not process url with ?$skip=100000 ath the end - returns same data as first link
    target_url: str,
):  # TODO -&gt; How to define Bag for type hinting? (https://docs.python.org/3/library/typing.html#newtype)
    &#34;&#34;&#34;Retrieves a table from a specific url for CBS Odata v4.

    Parameters
    ----------
    url_table_properties: str
        The url of the desired table

    Returns
    -------
    data: Dask bag
        All data received from target url as json type, in a Dask bag
    &#34;&#34;&#34;

    # First call target url and get json formatted response as dict
    temp_file = &#34;odata.json&#34;
    # r = requests.get(target_url).json()
    subprocess.run([&#34;curl&#34;, &#34;-fL&#34;, &#34;-o&#34;, temp_file, target_url])
    # Parse json file as dict
    with open(temp_file) as f:
        r = json.load(f)
    # Create Dask bag from dict
    bag = db.from_sequence(r[&#34;value&#34;])  # TODO -&gt; define npartitions?

    # check if more data exists
    if &#34;@odata.nextLink&#34; in r:
        target_url = r[&#34;@odata.nextLink&#34;]
    else:
        target_url = None

    # if more data exists continue to concat bag until complete
    while target_url:
        subprocess.run([&#34;curl&#34;, &#34;-fL&#34;, &#34;-o&#34;, temp_file, target_url])
        # Parse json file as dict
        with open(temp_file) as f:
            r = json.load(f)
        temp_bag = db.from_sequence(r[&#34;value&#34;])
        bag = db.concat([bag, temp_bag])

        if &#34;@odata.nextLink&#34; in r:
            target_url = r[&#34;@odata.nextLink&#34;]
        else:
            target_url = None

    return bag


def get_odata(target_url: str, odata_version: str):
    &#34;&#34;&#34;Gets a table from a valid CBS url and returns it as a Dask bag.

    A wrapper, calling the appropriate version function to get the table from
    a valid CBS url, leading to a table in OData format.

    Parameters
    ----------
    target_url : str
        A url to the table
    odata_version : str
        version of the odata for this dataset - must be either &#34;v3&#34; or &#34;v4&#34;.

    Returns
    -------
    Dask Bag
        All data received from target url as json type, concatenated in a Dask Bag

    Raises
    ------
    ValueError
        If &#39;odata_version` is not one of {&#34;v3&#34;, &#34;v4&#34;}
    &#34;&#34;&#34;

    if odata_version == &#34;v4&#34;:
        return get_odata_v4(target_url)
    elif odata_version == &#34;v3&#34;:
        return get_odata_v3(target_url)
    else:
        raise ValueError(&#34;odata version must be either &#39;v3&#39; or &#39;v4&#39;&#34;)


def get_odata_v3(target_url: str):
    &#34;&#34;&#34;Gets a table from a valid url for CBS dataset with Odata v3.

    This function uses standard requests.get() to retrieve data at target_url
    in json format, and concats it all into a Dask Bag to handle memory
    overflow if needed.

    Each request from CBS is limited to 10,000 rows, and if more data exists
    the key &#34;odata.nextLink&#34; exists in the response with the link to the next
    10,000 (or less) rows.

    Meant to be wrapped by `get_odata()`, and it is better practice to call it
    wrapped to allow for both &#34;v3&#34; and &#34;v4&#34; functionality.

    Parameters
    ----------
    target_url: str
        A valid url of a table from CBS

    Returns
    -------
    bag: Dask Bag
        All data received from target url as json type, concatenated as a Dask bag
    &#34;&#34;&#34;

    print(f&#34;Fetching from {target_url}&#34;)
    # First call target url and get json formatted response as dict
    r = requests.get(target_url).json()

    # Initialize bag as None
    bag = None

    # Create Dask bag from dict (check if not empty field)
    if r[&#34;value&#34;]:
        bag = db.from_sequence(r[&#34;value&#34;])  # TODO -&gt; define npartitions?

    # check if more data exists
    if &#34;odata.nextLink&#34; in r:
        target_url = r[&#34;odata.nextLink&#34;]
    else:
        target_url = None

    # if more data exists continue to concat bag until complete
    while target_url:
        r = requests.get(target_url).json()
        if r[&#34;value&#34;]:
            temp_bag = db.from_sequence(r[&#34;value&#34;])
            bag = db.concat([bag, temp_bag])

        if &#34;odata.nextLink&#34; in r:
            target_url = r[&#34;odata.nextLink&#34;]
        else:
            target_url = None

    return bag


def get_odata_v4(
    target_url: str,
):  # TODO -&gt; How to define Bag for type hinting? (maybe here: https://docs.python.org/3/library/typing.html#newtype)
    &#34;&#34;&#34;Gets a table from a specific url for CBS Odata v4.
    This function uses standard requests.get() to retrieve data at target_url
    in json format, and concats it all into a Dask Bag to handle memory
    overflow if needed.

    Each request from CBS is limited to 10,000 rows, and if more data exists
    the key &#34;@odata.nextLink&#34; exists in the response with the link to the next
    10,000 (or less) rows.

    Meant to be wrapped by `get_odata()`, and it is better practice to call it
    wrapped to allow for both &#34;v3&#34; and &#34;v4&#34; functionality.

    Parameters
    ----------
    target_url: str
        A valid url of a table from CBS

    Returns
    -------
    bag: Dask Bag
        All data received from target url as json type, concatenated as a Dask bag
    &#34;&#34;&#34;

    print(f&#34;Fetching from {target_url}&#34;)
    # First call target url and get json formatted response as dict
    r = requests.get(target_url).json()
    # Create Dask bag from dict
    bag = db.from_sequence(r[&#34;value&#34;])  # TODO -&gt; define npartitions?

    # check if more data exists
    if &#34;@odata.nextLink&#34; in r:
        target_url = r[&#34;@odata.nextLink&#34;]
    else:
        target_url = None

    # if more data exists continue to concat bag until complete
    while target_url:
        r = requests.get(target_url).json()
        temp_bag = db.from_sequence(r[&#34;value&#34;])
        bag = db.concat([bag, temp_bag])

        if &#34;@odata.nextLink&#34; in r:
            target_url = r[&#34;@odata.nextLink&#34;]
        else:
            target_url = None

    return bag


def convert_table_to_parquet(
    bag, file_name: str, out_dir: Union[str, Path]
) -&gt; Path:  # (TODO -&gt; IS THERE A FASTER/BETTER WAY??)
    &#34;&#34;&#34;Converts a Dask Bag to Parquet files and stores them on disk.

    Converts a dask bag holding data from a CBS table to Parquet form
    and stores it on disk. The bag should be filled by dicts (can be nested)
    which can be serialized as json.

    The current implementation iterates over each bag partition and dumps
    it into a json file, then appends all file into a single json file. That
    json file is then read into a PyArrow table, and finally that table is
    written as a parquet file to disk.

    Parameters
    ----------
    bag: Dask Bag
        A Bag holding (possibly nested) dicts that can serialized as json
    file_name: str)
        The name of the file to store on disk
    out_dir: str or Path
        A path to the directory where the file is stored

    Returns
    -------
    pq_path: Path
        The path to the output parquet file
    &#34;&#34;&#34;

    # create directories to store files
    out_dir = Path(out_dir)
    temp_json_dir = Path(&#34;./temp/json&#34;)
    create_dir(temp_json_dir)
    create_dir(out_dir)

    # File path to dump table as ndjson
    json_path = Path(f&#34;{temp_json_dir}/{file_name}.json&#34;)
    # File path to create as parquet file
    pq_path = Path(f&#34;{out_dir}/{file_name}.parquet&#34;)

    # Dump each bag partition to json file
    bag.map(json.dumps).to_textfiles(temp_json_dir / &#34;*.json&#34;)
    # Get all json file names with path
    filenames = sorted(glob(str(temp_json_dir) + &#34;/*.json&#34;))
    # Append all jsons into a single file  ## Also possible to use Dask Delayed here https://stackoverflow.com/questions/39566809/writing-dask-partitions-into-single-file
    with open(json_path, &#34;w+&#34;) as json_file:
        for fn in filenames:
            with open(fn) as f:
                json_file.write(f.read())
            remove(fn)

    # # Works without converting to ndjson - might be needed in a different implementation?
    # # Convert to ndjson format
    # with open(json_path, &#39;w+&#39;) as ndjson:
    #     for record in table:
    #         ndjson.write(json.dumps(record) + &#34;\n&#34;)

    # Create PyArrow table from ndjson file
    pa_table = pa_json.read_json(json_path)

    # Store parquet table #TODO -&gt; set proper data types in parquet file
    pq.write_table(pa_table, pq_path)

    # Remove temp ndjson file
    remove(json_path)
    # Remove temp folder if empty  #TODO -&gt; inefficiently(?) creates and removes the folder each time the function is called
    if not listdir(temp_json_dir):
        rmdir(temp_json_dir)
    return pq_path


def upload_to_gcs(
    dir: Path,
    source: str = &#34;cbs&#34;,
    odata_version: str = None,
    id: str = None,
    config: Config = None,
):
    &#34;&#34;&#34;Uploads all files in a given directory to Google Cloud Storage.

    This function is meant to be used for uploading all tables of a certain dataset retrieved from
    the CBS API. It therefore uses the following naming structure as the GCS blobs:

        &#34;{project_name}/{bucket_name}/{source}/{odata_version}/{id}/{YYYYMMDD}/{filename}&#34;

    For example, dataset &#34;82807NED&#34;, uploaded on Novemeber 11, 2020, to the &#34;dataverbinders&#34; project,
    using &#34;dataverbinders&#34; as a bucket, would create the following:

    - &#34;dataverbinders/dataverbinders/cbs/v4/83765NED/20201104/cbs.82807NED_Observations.parquet&#34;
    - &#34;dataverbinders/dataverbinders/cbs/v4/83765NED/20201104/cbs.82807NED_PeriodenCodes.parquet&#34;
    - etc..

    Parameters
    ----------
    dir: Path
        A Path object to a directory containing files to be uploaded
    source: str, default=&#34;cbs&#34;
        The source of the dataset. Currently only &#34;cbs&#34; is relevant.
    odata_version: str
        version of the odata for this dataset - must be either &#34;v3&#34; or &#34;v4&#34;.
    id: str
        CBS Dataset id, i.e. &#34;83583NED&#34;

    Returns
    -------
    gcs_folder: str
        The folder (=blob) into which the tables have been uploaded # TODO -&gt; Return success/ fail code?/job ID
    &#34;&#34;&#34;

    # Initialize Google Storage Client, get bucket, set blob
    gcs = storage.Client(
        project=config.gcp.dev.project_id
    )  # TODO -&gt; handle dev, test and prod appropriatley
    gcs_bucket = gcs.get_bucket(config.gcp.dev.bucket)
    gcs_folder = (
        f&#34;{source}/{odata_version}/{id}/{datetime.today().date().strftime(&#39;%Y%m%d&#39;)}&#34;
    )
    # Upload file
    for pfile in listdir(dir):
        gcs_blob = gcs_bucket.blob(gcs_folder + &#34;/&#34; + pfile)
        gcs_blob.upload_from_filename(
            dir / pfile
        )  # TODO: job currently returns None. Also how to handle if we get it?

    return gcs_folder  # TODO: return job id, if possible


def get_file_names(paths: Iterable[Union[str, PathLike]]) -&gt; list:
    &#34;&#34;&#34;Gets the filenames from an iterable of Path-like objects

    Parameters
    ----------
    paths: iterable of strings or path-like objects
        An iterable holding path-strings or path-like objects

    Returns
    -------
    file_names: list of str
        A list holding the extracted file names

    Example
    -------
    &gt;&gt;&gt; from pathlib import Path

    &gt;&gt;&gt; path1 = Path(&#39;some_folder/other_folder/some_file.txt&#39;)
    &gt;&gt;&gt; path2 = &#39;some_folder/different_folder/another_file.png&#39;
    &gt;&gt;&gt; full_paths = [path1, path2]

    &gt;&gt;&gt; file_names = get_file_names(full_paths)

    &gt;&gt;&gt; for name in file_names:
            print(name)
    some_file.txt
    another_file.png
    &#34;&#34;&#34;

    paths = [Path(path) for path in paths]
    file_names = [path.name for path in paths]
    return file_names


def cbsodata_to_gbq(
    id: str,
    odata_version: str,
    third_party: bool = False,
    source: str = &#34;cbs&#34;,
    config: Config = None,
):
    &#34;&#34;&#34;Loads a CBS dataset as a dataset in Google BigQuery.

    Retrieves a given dataset id from CBS, and converts it locally to Parquet. The
    Parquet files are uploaded to Google Cloud Storage, and a dataset is created
    in Google BigQuery, under which each permanenet tables are nested,linked to the
    Parquet files - each being a table of the dataset.

    Parameters
    ---------
    id: str
        CBS Dataset id, i.e. &#34;83583NED&#34;

    odata_version: str
        version of the odata for this dataset - must be either &#34;v3&#34; or &#34;v4&#34;.

    third_party: bool, default=False
        Flag to indicate dataset is not originally from CBS. Set to true
        to use dataderden.cbs.nl as base url (not available in v4 yet).

    source: str, default=&#34;cbs&#34;
        The source of the dataset. Currently only &#34;cbs&#34; is relevant.

    config: Config object
        Config object holding GCP and local paths

    Returns
    -------
    files_parquet: set of Paths
        A set with paths of local parquet files # TODO: replace with BQ job ids

    Example
    -------
    &gt;&gt;&gt; from statline_bq.utils import check_v4, cbsodata_to_gbq
    &gt;&gt;&gt; from statline_bq.config import get_config
    &gt;&gt;&gt; id = &#34;83583NED&#34;
    &gt;&gt;&gt; config = get_config(&#34;path/to/config.file&#34;)
    &gt;&gt;&gt; print(f&#34;Processing dataset {id}&#34;)
    &gt;&gt;&gt; odata_version = check_v4(id=id)
    &gt;&gt;&gt; cbsodata_to_gbq(
    ... id=id,
    ... odata_version=odata_version,
    ... config=config
    ... )
    &gt;&gt;&gt; print(f&#34;Completed dataset {id}&#34;)
    Processing dataset 83583NED
    # More messages from depending on internal process
    Completed dataset 83583NED

    Notes
    -----
    In **GCS**, the following &#34;folders&#34; and filenames structure is used:

        &#34;{project_name}/{bucket_name}/{source}/{version}/{dataset_id}/{date_of_upload}/{source}.{version}.{dataset_id}_{table_name}.parquet&#34;

    for example:

        &#34;dataverbinders/dataverbinders/cbs/v3/84286NED/20201125/cbs.v3.84286NED_TypedDataSet.parquet&#34;
    _________
    In **BQ**, the following structure and table names are used:

        &#34;[project/]/{source}_{version}_{dataset_id}/{dataset_id}/{table_name}&#34;

    for example:

        &#34;[dataverbinders/]/cbs_v3_83765NED/83765NED_Observations&#34;

    Odata version 3
    ---------------

    For given dataset id, the following tables are uploaded into GCS and linked in
    GBQ (taking `cbs` as default and `83583NED` as example):

    - &#34;cbs.v3.83583NED_DataProperties&#34; - Description of topics and dimensions contained in table
    - &#34;cbs.v3.83583NED_{DimensionName}&#34; - Separate dimension tables
    - &#34;cbs.v3.83583NED_TypedDataSet&#34; - The TypedDataset (***main table***)
    - &#34;cbs.v3.83583NED_CategoryGroups&#34; - Grouping of dimensions

    See *Handleiding CBS Open Data Services (v3)*[^odatav3] for details.

    Odata Version 4
    ---------------

    For a given dataset id, the following tables are ALWAYS uploaded into GCS
    and linked in GBQ (taking `cbs` as default and `83765NED` as example):

    - &#34;cbs.v4.83765NED_Observations&#34; - The actual values of the dataset (***main table***)
    - &#34;cbs.v4.83765NED_MeasureCodes&#34; - Describing the codes that appear in the Measure column of the Observations table.
    - &#34;cbs.v4.83765NED_Dimensions&#34; - Information regarding the dimensions

    Additionally, this function will upload all other tables related to the dataset, except for `Properties`.
        
    These may include:

    - &#34;cbs.v4.83765NED_MeasureGroups&#34; - Describing the hierarchy of the Measures

    And, for each Dimension listed in the `Dimensions` table (i.e. `{Dimension_1}`)
    
    - &#34;cbs.v4.83765NED_{Dimension_1}Codes&#34;
    - &#34;cbs.v4.83765NED_{Dimension_1}Groups&#34; (IF IT EXISTS)

    See *Informatie voor Ontwikelaars*[^odatav4] for details.

    [^odatav3]: https://www.cbs.nl/-/media/statline/documenten/handleiding-cbs-ewopendata-services.pdf
    [^odatav4]: https://dataportal.cbs.nl/info/ontwikkelaars
    &#34;&#34;&#34;

    # Get all tablbe urls for given dataset id
    urls = get_urls(id=id, odata_version=odata_version, third_party=third_party)
    # Create directory to store parquest files locally
    pq_dir = create_named_dir(
        id=id, odata_version=odata_version, source=source, config=config
    )
    # fetch each table from urls, convert to parquet and store locally
    files_parquet = tables_to_parquet(
        id=id, urls=urls, odata_version=odata_version, source=source, pq_dir=pq_dir
    )
    # Get the description of the data set from CBS
    description_text = get_dataset_description(urls, odata_version=odata_version)
    # Write description text to txt file and store in dataset directory with parquet tables
    write_description_to_file(
        id=id,
        description_text=description_text,
        pq_dir=pq_dir,
        source=source,
        odata_version=odata_version,
    )

    # Upload to GCS
    gcs_folder = upload_to_gcs(pq_dir, source, odata_version, id, config)

    # Keep only names
    file_names = get_file_names(files_parquet)
    # Create table in GBQ
    gcs_to_gbq(
        id=id,
        source=source,
        odata_version=odata_version,
        third_party=third_party,
        config=config,
        gcs_folder=gcs_folder,
        file_names=file_names,
    )

    return files_parquet  # TODO: return bq job ids


def get_urls(id: str, odata_version: str, third_party: bool = False):
    &#34;&#34;&#34;Returns a dict with urls of all dataset tables given a valid CBS dataset id.

    Parameters
    ----------
    id: str
        CBS Dataset id, i.e. &#34;83583NED&#34;

    odata_version: str
        version of the odata for this dataset - must be either &#34;v3&#34; or &#34;v4&#34;.

    third_party: bool, default=False
        Flag to indicate dataset is not originally from CBS. Set to true
        to use dataderden.cbs.nl as base url (not available in v4 yet).

    Returns:
    urls: dict of str
        A dict containing all urls of a CBS dataset&#39;s tables

    Examples:
    &gt;&gt;&gt; dataset_id = &#39;83583NED&#39;
    &gt;&gt;&gt; urls = get_urls(id=dataset_id, odata_version=&#34;v3&#34;, third_party=False)
    &gt;&gt;&gt; for name, url in urls.items():
    ...     print(f&#34;{name}: {url}&#34;)
    TableInfos: https://opendata.cbs.nl/ODataFeed/odata/83583NED/TableInfos
    UntypedDataSet: https://opendata.cbs.nl/ODataFeed/odata/83583NED/UntypedDataSet
    TypedDataSet: https://opendata.cbs.nl/ODataFeed/odata/83583NED/TypedDataSet
    DataProperties: https://opendata.cbs.nl/ODataFeed/odata/83583NED/DataProperties
    CategoryGroups: https://opendata.cbs.nl/ODataFeed/odata/83583NED/CategoryGroups
    BedrijfstakkenBranchesSBI2008: https://opendata.cbs.nl/ODataFeed/odata/83583NED/BedrijfstakkenBranchesSBI2008
    Bedrijfsgrootte: https://opendata.cbs.nl/ODataFeed/odata/83583NED/Bedrijfsgrootte
    Perioden: https://opendata.cbs.nl/ODataFeed/odata/83583NED/Perioden 
    &#34;&#34;&#34;

    if odata_version == &#34;v4&#34;:
        base_url = {
            True: None,  # currently no IV3 links in ODATA V4,
            False: f&#34;https://odata4.cbs.nl/CBS/{id}&#34;,
        }
        urls = {
            item[&#34;name&#34;]: base_url[third_party] + &#34;/&#34; + item[&#34;url&#34;]
            for item in get_odata_v4(base_url[third_party])
        }
    elif odata_version == &#34;v3&#34;:
        base_url = {
            True: f&#34;https://dataderden.cbs.nl/ODataFeed/odata/{id}?$format=json&#34;,
            False: f&#34;https://opendata.cbs.nl/ODataFeed/odata/{id}?$format=json&#34;,
        }
        urls = {
            item[&#34;name&#34;]: item[&#34;url&#34;]
            for item in requests.get(base_url[third_party]).json()[&#34;value&#34;]
        }
    else:
        raise ValueError(&#34;odata version must be either &#39;v3&#39; or &#39;v4&#39;&#34;)
    return urls


def create_named_dir(
    id: str, odata_version: str, source: str = &#34;cbs&#34;, config: Config = None
):
    &#34;&#34;&#34;Creates a directory according to a specific structure.

    A convenience function, creatind a directory according to the following
    pattern, based on a config object and the rest of the parameters. Meant to
    create a directory for each dataset where its related tables are written
    into as parquet files.

    Directory pattern:
        
        &#34;~/{config.paths.root}/{config.paths.temp}/{source}/{id}/{date_of_creation}/parquet&#34;

    Parameters
    ----------
    id: str
        CBS Dataset id, i.e. &#34;83583NED&#34;
    odata_version: str
        version of the odata for this dataset - must be either &#34;v3&#34; or &#34;v4&#34;.
    source: str, default=&#34;cbs&#34;
        The source of the dataset. Currently only &#34;cbs&#34; is relevant.
    config: Config object
        Config object holding GCP and local paths

    Returns
    -------
    path_to_named_dir: Path
        path to created folder

    -------
    Example
    -------
    &gt;&gt;&gt; from statline_bq.utils import create_named_dir
    &gt;&gt;&gt; from statline_bq.config import get_config
    &gt;&gt;&gt; id = &#34;83583NED&#34;
    &gt;&gt;&gt; config = get_config(&#34;path/to/config.file&#34;)
    &gt;&gt;&gt; print(config.paths.root)
    Projects/statline-bq
    &gt;&gt;&gt; print(config.paths.temp)
    temp
    &gt;&gt;&gt; dir = create_named_dir(id=id, odata_version=&#34;v3&#34;)
    &gt;&gt;&gt; dir
    PosixPath(&#39;/Users/{YOUR_USERNAME}/Projects/statline-bq/temp/cbs/v3/83583NED/20201214/parquet&#39;)
    &#34;&#34;&#34;

    # Get paths from config object
    root = Path.home() / Path(config.paths.root)
    temp = root / Path(config.paths.temp)

    # Create placeholders for storage
    path = temp / Path(
        f&#34;{source}/{odata_version}/{id}/{datetime.today().date().strftime(&#39;%Y%m%d&#39;)}/parquet&#34;
    )
    path_to_named_dir = create_dir(path)
    return path_to_named_dir


def tables_to_parquet(
    id: str,
    urls: dict,
    odata_version: str,
    source: str = &#34;cbs&#34;,
    pq_dir: Union[Path, str] = None,
) -&gt; set:
    &#34;&#34;&#34;Downloads all tables related to a valid CBS dataset id, and stores them locally as Parquet files.

    Parameters
    ----------
    id : str
        CBS Dataset id, i.e. &#34;83583NED&#34;
    urls : dict
        Dictionary holding urls of all dataset tables from CBS
    odata_version : str
        version of the odata for this dataset - must be either &#34;v3&#34; or &#34;v4&#34;.
    source : str, default=&#39;cbs
        The source of the dataset. Currently only &#34;cbs&#34; is relevant.
    pq_dir : Path or str
        The directory where the putput Parquet files are stored.

    Returns
    -------
    files_parquet: set of Path
        A set containing Path objects of all output Parquet files
    &#34;&#34;&#34;

    # Create placeholders for storage
    files_parquet = set()

    # Iterate over all tables related to dataset, excepet Properties (from v4), TableInfos (from v3) and UntypedDataset (from v3) (TODO -&gt; double check that it is redundandt)
    for key, url in [
        (k, v)
        for k, v in urls.items()
        if k
        not in (
            &#34;Properties&#34;,
            &#34;TableInfos&#34;,
            &#34;UntypedDataSet&#34;,
        )  # Redundant tables from v3 AND v4
    ]:

        # for v3 urls an appendix of &#34;?format=json&#34; is needed
        if odata_version == &#34;v3&#34;:
            url = &#34;?&#34;.join((url, &#34;$format=json&#34;))

        # Create table name to be used in GCS
        table_name = f&#34;{source}.{odata_version}.{id}_{key}&#34;

        # Get data from source
        table = get_odata(target_url=url, odata_version=odata_version)

        # Check if get_odata returned None (when link in CBS returns empty table, i.e. CategoryGroups in &#34;84799NED&#34; - seems only relevant for v3 only)
        if table is not None:

            # Convert to parquet
            pq_path = convert_table_to_parquet(table, table_name, pq_dir)

            # Add path of file to set
            files_parquet.add(pq_path)

    return files_parquet


def create_bq_dataset(
    id: str,
    source: str = &#34;cbs&#34;,
    odata_version: str = None,
    description: str = None,
    gcp: Gcp = None,
) -&gt; str:
    &#34;&#34;&#34;Creates a dataset in Google Big Query. If dataset exists already exists, does nothing.

    Parameters
    ----------
    id: str
        CBS Dataset id, i.e. &#34;83583NED&#34;.
    source: str, default=&#34;cbs&#34;
        The source of the dataset. Currently only &#34;cbs&#34; is relevant.
    odata_version: str
        version of the odata for this dataset - must be either &#34;v3&#34; or &#34;v4&#34;.
    description: str
        The description of the dataset
    gcp: Gcp
        A Gcp Class object, holding GCP parameters

    Returns:
    dataset.dataset_id: str
        The id of the created BQ dataset
    &#34;&#34;&#34;

    # Construct a BigQuery client object.
    client = bigquery.Client(project=gcp.dev.project_id)

    # Set dataset_id to the ID of the dataset to create.
    dataset_id = f&#34;{client.project}.{source}_{odata_version}_{id}&#34;

    # Construct a full Dataset object to send to the API.
    dataset = bigquery.Dataset(dataset_id)

    # Specify the geographic location where the dataset should reside.
    dataset.location = gcp.dev.location

    # Add description if provided
    dataset.description = description

    # Send the dataset to the API for creation, with an explicit timeout.
    # Raises google.api_core.exceptions.Conflict if the Dataset already
    # exists within the project.
    try:
        dataset = client.create_dataset(dataset, timeout=30)  # Make an API request.
        print(f&#34;Created dataset {client.project}.{dataset.dataset_id}&#34;)
    except exceptions.Conflict:
        print(f&#34;Dataset {client.project}.{dataset.dataset_id} already exists&#34;)
    finally:
        return dataset.dataset_id


def check_bq_dataset(id: str, source: str, odata_version: str, gcp: Gcp = None) -&gt; bool:
    &#34;&#34;&#34;Check if dataset exists in BQ.

    Parameters:
        - id (str): the dataset id, i.e. &#39;83583NED&#39;
        - source (str): source to load data into
        - odata_version (str): &#34;v3&#34; or &#34;v4&#34; indicating the version
        - gcp (Gcp): a config object

    Returns:
        - True if exists, False if does not exists
    &#34;&#34;&#34;
    client = bigquery.Client(project=gcp.dev.project_id)

    dataset_id = f&#34;{source}_{odata_version}_{id}&#34;

    try:
        client.get_dataset(dataset_id)  # Make an API request.
        # print(f&#34;Dataset {dataset_id} already exists&#34;)
        return True
    except exceptions.NotFound:
        # print(f&#34;Dataset {dataset_id} is not found&#34;
        return False


def delete_bq_dataset(
    id: str, source: str = &#34;cbs&#34;, odata_version: str = None, gcp: Gcp = None
) -&gt; None:
    &#34;&#34;&#34;Delete an exisiting dataset from Google Big Query

    Parameters
    ----------
    id: str
        CBS Dataset id, i.e. &#34;83583NED&#34;.
    source: str, default=&#34;cbs&#34;
        The source of the dataset. Currently only &#34;cbs&#34; is relevant.
    odata_version: str
        version of the odata for this dataset - must be either &#34;v3&#34; or &#34;v4&#34;.
    gcp: Gcp
        A Gcp Class object, holding GCP parameters

    Returns
    -------
    None
    &#34;&#34;&#34;

    # Construct a bq client
    client = bigquery.Client(project=gcp.dev.project_id)

    # Set bq dataset id string
    dataset_id = f&#34;{source}_{odata_version}_{id}&#34;

    # Delete the dataset and its contents
    client.delete_dataset(dataset_id, delete_contents=True, not_found_ok=True)

    return None


def get_description_from_gcs(
    id: str,
    source: str = &#34;cbs&#34;,
    odata_version: str = None,
    gcp: Gcp = None,
    gcs_folder: str = None,
) -&gt; str:
    &#34;&#34;&#34;Gets previsouly uploaded dataset description from GCS.

    The description should exist in the following file, under the following structure:

        &#34;{project}/{bucket}/{source}/{odata_version}/{id}/{YYYYMMDD}/{source}.{odata_version}.{id}_Description&#34;

    For example:

        &#34;dataverbinders-dev/cbs/v4/83765NED/20201127/cbs.v4.83765NED_Description.txt&#34;

    Parameters
    ----------
    id: str
        CBS Dataset id, i.e. &#34;83583NED&#34;.
    source: str, default=&#34;cbs&#34;
        The source of the dataset. Currently only &#34;cbs&#34; is relevant.
    odata_version: str
        version of the odata for this dataset - must be either &#34;v3&#34; or &#34;v4&#34;.
    gcp: Gcp
        A Gcp Class object, holding GCP parameters
    gcs_folder : str
        The GCS folder holding the description txt file

    Returns
    -------
    str
        [description]
    &#34;&#34;&#34;

    client = storage.Client(project=gcp.dev.project_id)
    bucket = client.get_bucket(gcp.dev.bucket)
    blob = bucket.get_blob(
        f&#34;{gcs_folder}/{source}.{odata_version}.{id}_Description.txt&#34;
    )
    return str(blob.download_as_string().decode(&#34;utf-8&#34;))


def gcs_to_gbq(
    id: str,
    source: str = &#34;cbs&#34;,
    odata_version: str = None,
    third_party: bool = False,
    config: Config = None,
    gcs_folder: str = None,
    file_names: list = None,
):
    &#34;&#34;&#34;Creates a BQ dataset and links all relevant tables from GCS underneath.
    
    Creates a dataset (if does not exist) in Google Big Query, and underneath
    creates permanent tables linked to parquet file stored in Google Storage.
    If dataset exists, removes it and recreates it with most up to date uploaded files (?) # TODO: Is this the best logic?
    
    Parameters
    ----------
    id: str
        CBS Dataset id, i.e. &#34;83583NED&#34;.
    source: str, default=&#34;cbs&#34;
        The source of the dataset. Currently only &#34;cbs&#34; is relevant.
    odata_version: str
        version of the odata for this dataset - must be either &#34;v3&#34; or &#34;v4&#34;.
    third_party : bool, default=False
        Flag to indicate dataset is not originally from CBS. Set to true to use dataderden.cbs.nl as base url (not available in v4 yet).
    config : Config object
        Config object holding GCP and local paths.
    gcs_folder : str
        The GCS folder holding the description txt file.
    file_names : list
        A list holding all file names of tables to be linked.


    Returns
    -------
    # TODO Return job id
        [description]
    &#34;&#34;&#34;

    # # Get all parquet files in gcs folder from GCS
    # storage_client = storage.Client(project=gcp.dev.project_id)

    # TODO: retrieve names from GCS? If yes, loop below should change to use these two lists
    # blob_uris = [
    #     blob.self_link
    #     for blob in storage_client.list_blobs(gcp.dev.bucket, prefix=gcs_folder)
    #     if not blob.name.endswith(&#34;.txt&#34;)
    # ]
    # blob_names = [
    #     blob.name
    #     for blob in storage_client.list_blobs(gcp.dev.bucket, prefix=gcs_folder)
    #     if not blob.name.endswith(&#34;.txt&#34;)
    # ]

    # Get description text from txt file
    description = get_description_from_gcs(
        id=id,
        source=source,
        odata_version=odata_version,
        gcp=config.gcp,
        gcs_folder=gcs_folder,
    )

    # Check if dataset exists and delete if it does TODO: maybe delete anyway (deleting uses not_found_ok to ignore error if does not exist)
    if check_bq_dataset(
        id=id, source=source, odata_version=odata_version, gcp=config.gcp
    ):
        delete_bq_dataset(
            id=id, source=source, odata_version=odata_version, gcp=config.gcp
        )

    # Create a dataset in BQ
    dataset_id = create_bq_dataset(
        id=id,
        source=source,
        odata_version=odata_version,
        description=description,
        gcp=config.gcp,
    )
    # if not existing:
    # Skip?
    # else:
    # Handle existing dataset - delete and recreate? Repopulate? TODO

    # Initialize client
    client = bigquery.Client(project=config.gcp.dev.project_id)

    # Configure the external data source
    # dataset_id = f&#34;{source}_{odata_version}_{id}&#34;
    dataset_ref = bigquery.DatasetReference(config.gcp.dev.project_id, dataset_id)

    # Loop over all files related to this dataset id
    for name in file_names:
        # Configure the external data source per table id
        table_id = str(name).split(&#34;.&#34;)[2]
        table = bigquery.Table(dataset_ref.table(table_id))

        external_config = bigquery.ExternalConfig(&#34;PARQUET&#34;)
        external_config.source_uris = [
            f&#34;https://storage.cloud.google.com/{config.gcp.dev.bucket}/{gcs_folder}/{name}&#34;  # TODO: Handle dev/test/prod?
        ]
        table.external_data_configuration = external_config
        # table.description = description

        # Create a permanent table linked to the GCS file
        table = client.create_table(
            table, exists_ok=True
        )  # BUG: error raised, using exists_ok=True to avoid
    return None  # TODO Return job id


def main(
    id: str, source: str = &#34;cbs&#34;, third_party: bool = False, config: Config = None,
):
    print(f&#34;Processing dataset {id}&#34;)
    odata_version = check_v4(id=id, third_party=third_party)
    cbsodata_to_gbq(
        id=id,
        odata_version=odata_version,
        third_party=third_party,
        source=source,
        config=config,
    )
    print(
        f&#34;Completed dataset {id}&#34;
    )  # TODO - add response from google if possible (some success/failure flag)
    return None


if __name__ == &#34;__main__&#34;:
    from statline_bq.config import get_config

    config = get_config(&#34;./statline_bq/config.toml&#34;)
    main(&#34;83583NED&#34;, config=config)

# from statline_bq.config import get_config

# config = get_config(&#34;./config.toml&#34;)

# description = get_description_v3(
#     &#34;https://opendata.cbs.nl/ODataFeed/odata/{id}?$format=json&#34;
# )
# print(description)

# gcs_to_gbq(
#     # id=&#34;835833NED&#34;,
#     source=&#34;cbs&#34;,
#     odata_version=&#34;v3&#34;,
#     gcp=config.gcp,
#     gcs_folder=&#34;cbs/v3/83583NED/20201126&#34;,
#     file_names=[&#34;cbs.v3.83583NED_Bedrijfsgrootte.parquet&#34;],
# )</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="statline_bq.utils.cbsodata_to_gbq"><code class="name flex">
<span>def <span class="ident">cbsodata_to_gbq</span></span>(<span>id: str, odata_version: str, third_party: bool = False, source: str = 'cbs', config: <a title="statline_bq.config.Config" href="config.html#statline_bq.config.Config">Config</a> = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Loads a CBS dataset as a dataset in Google BigQuery.</p>
<p>Retrieves a given dataset id from CBS, and converts it locally to Parquet. The
Parquet files are uploaded to Google Cloud Storage, and a dataset is created
in Google BigQuery, under which each permanenet tables are nested,linked to the
Parquet files - each being a table of the dataset.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>id</code></strong> :&ensp;<code>str</code></dt>
<dd>CBS Dataset id, i.e. "83583NED"</dd>
<dt><strong><code>odata_version</code></strong> :&ensp;<code>str</code></dt>
<dd>version of the odata for this dataset - must be either "v3" or "v4".</dd>
<dt><strong><code>third_party</code></strong> :&ensp;<code>bool</code>, default=<code>False</code></dt>
<dd>Flag to indicate dataset is not originally from CBS. Set to true
to use dataderden.cbs.nl as base url (not available in v4 yet).</dd>
<dt><strong><code>source</code></strong> :&ensp;<code>str</code>, default=<code>"cbs"</code></dt>
<dd>The source of the dataset. Currently only "cbs" is relevant.</dd>
<dt><strong><code>config</code></strong> :&ensp;<code>Config object</code></dt>
<dd>Config object holding GCP and local paths</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>files_parquet</code></strong> :&ensp;<code>set</code> of <code>Paths</code></dt>
<dd>A set with paths of local parquet files # TODO: replace with BQ job ids</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code class="language-python">&gt;&gt;&gt; from statline_bq.utils import check_v4, cbsodata_to_gbq
&gt;&gt;&gt; from statline_bq.config import get_config
&gt;&gt;&gt; id = &quot;83583NED&quot;
&gt;&gt;&gt; config = get_config(&quot;path/to/config.file&quot;)
&gt;&gt;&gt; print(f&quot;Processing dataset {id}&quot;)
&gt;&gt;&gt; odata_version = check_v4(id=id)
&gt;&gt;&gt; cbsodata_to_gbq(
... id=id,
... odata_version=odata_version,
... config=config
... )
&gt;&gt;&gt; print(f&quot;Completed dataset {id}&quot;)
Processing dataset 83583NED
# More messages from depending on internal process
Completed dataset 83583NED
</code></pre>
<h2 id="notes">Notes</h2>
<p>In <strong>GCS</strong>, the following "folders" and filenames structure is used:</p>
<pre><code>"{project_name}/{bucket_name}/{source}/{version}/{dataset_id}/{date_of_upload}/{source}.{version}.{dataset_id}_{table_name}.parquet"
</code></pre>
<p>for example:</p>
<pre><code>"dataverbinders/dataverbinders/cbs/v3/84286NED/20201125/cbs.v3.84286NED_TypedDataSet.parquet"
</code></pre>
<hr>
<p>In <strong>BQ</strong>, the following structure and table names are used:</p>
<pre><code>"[project/]/{source}_{version}_{dataset_id}/{dataset_id}/{table_name}"
</code></pre>
<p>for example:</p>
<pre><code>"[dataverbinders/]/cbs_v3_83765NED/83765NED_Observations"
</code></pre>
<h2 id="odata-version-3">Odata Version 3</h2>
<p>For given dataset id, the following tables are uploaded into GCS and linked in
GBQ (taking <code>cbs</code> as default and <code>83583NED</code> as example):</p>
<ul>
<li>"cbs.v3.83583NED_DataProperties" - Description of topics and dimensions contained in table</li>
<li>"cbs.v3.83583NED_{DimensionName}" - Separate dimension tables</li>
<li>"cbs.v3.83583NED_TypedDataSet" - The TypedDataset (<strong><em>main table</em></strong>)</li>
<li>"cbs.v3.83583NED_CategoryGroups" - Grouping of dimensions</li>
</ul>
<p>See <em>Handleiding CBS Open Data Services (v3)</em><sup id="fnref:odatav3"><a class="footnote-ref" href="#fn:odatav3">1</a></sup> for details.</p>
<h2 id="odata-version-4">Odata Version 4</h2>
<p>For a given dataset id, the following tables are ALWAYS uploaded into GCS
and linked in GBQ (taking <code>cbs</code> as default and <code>83765NED</code> as example):</p>
<ul>
<li>"cbs.v4.83765NED_Observations" - The actual values of the dataset (<strong><em>main table</em></strong>)</li>
<li>"cbs.v4.83765NED_MeasureCodes" - Describing the codes that appear in the Measure column of the Observations table.</li>
<li>"cbs.v4.83765NED_Dimensions" - Information regarding the dimensions</li>
</ul>
<p>Additionally, this function will upload all other tables related to the dataset, except for <code>Properties</code>.</p>
<p>These may include:</p>
<ul>
<li>"cbs.v4.83765NED_MeasureGroups" - Describing the hierarchy of the Measures</li>
</ul>
<p>And, for each Dimension listed in the <code>Dimensions</code> table (i.e. <code>{Dimension_1}</code>)</p>
<ul>
<li>"cbs.v4.83765NED_{Dimension_1}Codes"</li>
<li>"cbs.v4.83765NED_{Dimension_1}Groups" (IF IT EXISTS)</li>
</ul>
<p>See <em>Informatie voor Ontwikelaars</em><sup id="fnref:odatav4"><a class="footnote-ref" href="#fn:odatav4">2</a></sup> for details.</p>
<div class="footnote">
<hr>
<ol>
<li id="fn:odatav3">
<p><a href="https://www.cbs.nl/-/media/statline/documenten/handleiding-cbs-ewopendata-services.pdf">https://www.cbs.nl/-/media/statline/documenten/handleiding-cbs-ewopendata-services.pdf</a>&#160;<a class="footnote-backref" href="#fnref:odatav3" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:odatav4">
<p><a href="https://dataportal.cbs.nl/info/ontwikkelaars">https://dataportal.cbs.nl/info/ontwikkelaars</a>&#160;<a class="footnote-backref" href="#fnref:odatav4" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cbsodata_to_gbq(
    id: str,
    odata_version: str,
    third_party: bool = False,
    source: str = &#34;cbs&#34;,
    config: Config = None,
):
    &#34;&#34;&#34;Loads a CBS dataset as a dataset in Google BigQuery.

    Retrieves a given dataset id from CBS, and converts it locally to Parquet. The
    Parquet files are uploaded to Google Cloud Storage, and a dataset is created
    in Google BigQuery, under which each permanenet tables are nested,linked to the
    Parquet files - each being a table of the dataset.

    Parameters
    ---------
    id: str
        CBS Dataset id, i.e. &#34;83583NED&#34;

    odata_version: str
        version of the odata for this dataset - must be either &#34;v3&#34; or &#34;v4&#34;.

    third_party: bool, default=False
        Flag to indicate dataset is not originally from CBS. Set to true
        to use dataderden.cbs.nl as base url (not available in v4 yet).

    source: str, default=&#34;cbs&#34;
        The source of the dataset. Currently only &#34;cbs&#34; is relevant.

    config: Config object
        Config object holding GCP and local paths

    Returns
    -------
    files_parquet: set of Paths
        A set with paths of local parquet files # TODO: replace with BQ job ids

    Example
    -------
    &gt;&gt;&gt; from statline_bq.utils import check_v4, cbsodata_to_gbq
    &gt;&gt;&gt; from statline_bq.config import get_config
    &gt;&gt;&gt; id = &#34;83583NED&#34;
    &gt;&gt;&gt; config = get_config(&#34;path/to/config.file&#34;)
    &gt;&gt;&gt; print(f&#34;Processing dataset {id}&#34;)
    &gt;&gt;&gt; odata_version = check_v4(id=id)
    &gt;&gt;&gt; cbsodata_to_gbq(
    ... id=id,
    ... odata_version=odata_version,
    ... config=config
    ... )
    &gt;&gt;&gt; print(f&#34;Completed dataset {id}&#34;)
    Processing dataset 83583NED
    # More messages from depending on internal process
    Completed dataset 83583NED

    Notes
    -----
    In **GCS**, the following &#34;folders&#34; and filenames structure is used:

        &#34;{project_name}/{bucket_name}/{source}/{version}/{dataset_id}/{date_of_upload}/{source}.{version}.{dataset_id}_{table_name}.parquet&#34;

    for example:

        &#34;dataverbinders/dataverbinders/cbs/v3/84286NED/20201125/cbs.v3.84286NED_TypedDataSet.parquet&#34;
    _________
    In **BQ**, the following structure and table names are used:

        &#34;[project/]/{source}_{version}_{dataset_id}/{dataset_id}/{table_name}&#34;

    for example:

        &#34;[dataverbinders/]/cbs_v3_83765NED/83765NED_Observations&#34;

    Odata version 3
    ---------------

    For given dataset id, the following tables are uploaded into GCS and linked in
    GBQ (taking `cbs` as default and `83583NED` as example):

    - &#34;cbs.v3.83583NED_DataProperties&#34; - Description of topics and dimensions contained in table
    - &#34;cbs.v3.83583NED_{DimensionName}&#34; - Separate dimension tables
    - &#34;cbs.v3.83583NED_TypedDataSet&#34; - The TypedDataset (***main table***)
    - &#34;cbs.v3.83583NED_CategoryGroups&#34; - Grouping of dimensions

    See *Handleiding CBS Open Data Services (v3)*[^odatav3] for details.

    Odata Version 4
    ---------------

    For a given dataset id, the following tables are ALWAYS uploaded into GCS
    and linked in GBQ (taking `cbs` as default and `83765NED` as example):

    - &#34;cbs.v4.83765NED_Observations&#34; - The actual values of the dataset (***main table***)
    - &#34;cbs.v4.83765NED_MeasureCodes&#34; - Describing the codes that appear in the Measure column of the Observations table.
    - &#34;cbs.v4.83765NED_Dimensions&#34; - Information regarding the dimensions

    Additionally, this function will upload all other tables related to the dataset, except for `Properties`.
        
    These may include:

    - &#34;cbs.v4.83765NED_MeasureGroups&#34; - Describing the hierarchy of the Measures

    And, for each Dimension listed in the `Dimensions` table (i.e. `{Dimension_1}`)
    
    - &#34;cbs.v4.83765NED_{Dimension_1}Codes&#34;
    - &#34;cbs.v4.83765NED_{Dimension_1}Groups&#34; (IF IT EXISTS)

    See *Informatie voor Ontwikelaars*[^odatav4] for details.

    [^odatav3]: https://www.cbs.nl/-/media/statline/documenten/handleiding-cbs-ewopendata-services.pdf
    [^odatav4]: https://dataportal.cbs.nl/info/ontwikkelaars
    &#34;&#34;&#34;

    # Get all tablbe urls for given dataset id
    urls = get_urls(id=id, odata_version=odata_version, third_party=third_party)
    # Create directory to store parquest files locally
    pq_dir = create_named_dir(
        id=id, odata_version=odata_version, source=source, config=config
    )
    # fetch each table from urls, convert to parquet and store locally
    files_parquet = tables_to_parquet(
        id=id, urls=urls, odata_version=odata_version, source=source, pq_dir=pq_dir
    )
    # Get the description of the data set from CBS
    description_text = get_dataset_description(urls, odata_version=odata_version)
    # Write description text to txt file and store in dataset directory with parquet tables
    write_description_to_file(
        id=id,
        description_text=description_text,
        pq_dir=pq_dir,
        source=source,
        odata_version=odata_version,
    )

    # Upload to GCS
    gcs_folder = upload_to_gcs(pq_dir, source, odata_version, id, config)

    # Keep only names
    file_names = get_file_names(files_parquet)
    # Create table in GBQ
    gcs_to_gbq(
        id=id,
        source=source,
        odata_version=odata_version,
        third_party=third_party,
        config=config,
        gcs_folder=gcs_folder,
        file_names=file_names,
    )

    return files_parquet  # TODO: return bq job ids</code></pre>
</details>
</dd>
<dt id="statline_bq.utils.check_bq_dataset"><code class="name flex">
<span>def <span class="ident">check_bq_dataset</span></span>(<span>id: str, source: str, odata_version: str, gcp: <a title="statline_bq.config.Gcp" href="config.html#statline_bq.config.Gcp">Gcp</a> = None) ‑> bool</span>
</code></dt>
<dd>
<div class="desc"><p>Check if dataset exists in BQ.</p>
<h2 id="parameters">Parameters</h2>
<ul>
<li>id (str): the dataset id, i.e. '83583NED'</li>
<li>source (str): source to load data into</li>
<li>odata_version (str): "v3" or "v4" indicating the version</li>
<li>gcp (Gcp): a config object</li>
</ul>
<h2 id="returns">Returns</h2>
<ul>
<li>True if exists, False if does not exists</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_bq_dataset(id: str, source: str, odata_version: str, gcp: Gcp = None) -&gt; bool:
    &#34;&#34;&#34;Check if dataset exists in BQ.

    Parameters:
        - id (str): the dataset id, i.e. &#39;83583NED&#39;
        - source (str): source to load data into
        - odata_version (str): &#34;v3&#34; or &#34;v4&#34; indicating the version
        - gcp (Gcp): a config object

    Returns:
        - True if exists, False if does not exists
    &#34;&#34;&#34;
    client = bigquery.Client(project=gcp.dev.project_id)

    dataset_id = f&#34;{source}_{odata_version}_{id}&#34;

    try:
        client.get_dataset(dataset_id)  # Make an API request.
        # print(f&#34;Dataset {dataset_id} already exists&#34;)
        return True
    except exceptions.NotFound:
        # print(f&#34;Dataset {dataset_id} is not found&#34;
        return False</code></pre>
</details>
</dd>
<dt id="statline_bq.utils.check_v4"><code class="name flex">
<span>def <span class="ident">check_v4</span></span>(<span>id: str, third_party: bool = False) ‑> str</span>
</code></dt>
<dd>
<div class="desc"><p>Checks whether a certain CBS table exists as OData Version "v4".</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>id</code></strong> :&ensp;<code>str</code></dt>
<dd>CBS Dataset id, i.e. "83583NED"</dd>
<dt><strong><code>third_party</code></strong> :&ensp;<code>bool</code>, default=<code>False</code></dt>
<dd>Flag to indicate dataset is not originally from CBS. Set to true
to use dataderden.cbs.nl as base url (not available in v4 yet).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>odata_version</code></strong> :&ensp;<code>str</code></dt>
<dd>"v4" if exists as odata v4, "v3" otherwise.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_v4(id: str, third_party: bool = False) -&gt; str:
    &#34;&#34;&#34;Checks whether a certain CBS table exists as OData Version &#34;v4&#34;.

    Parameters
    ----------
    id: str
        CBS Dataset id, i.e. &#34;83583NED&#34;

    third_party: bool, default=False
        Flag to indicate dataset is not originally from CBS. Set to true
        to use dataderden.cbs.nl as base url (not available in v4 yet).

    Returns
    -------
    odata_version: str
        &#34;v4&#34; if exists as odata v4, &#34;v3&#34; otherwise.
    &#34;&#34;&#34;

    base_url = {
        True: None,  # currently no IV3 links in ODATA V4,
        False: f&#34;https://odata4.cbs.nl/CBS/{id}&#34;,
    }
    r = requests.get(base_url[third_party])
    if (
        r.status_code == 200
    ):  # TODO: Is this the best check to use? Maybe if not 404? Or something else?
        odata_version = &#34;v4&#34;
    else:
        odata_version = &#34;v3&#34;
    return odata_version</code></pre>
</details>
</dd>
<dt id="statline_bq.utils.convert_table_to_parquet"><code class="name flex">
<span>def <span class="ident">convert_table_to_parquet</span></span>(<span>bag, file_name: str, out_dir: Union[str, pathlib.Path]) ‑> pathlib.Path</span>
</code></dt>
<dd>
<div class="desc"><p>Converts a Dask Bag to Parquet files and stores them on disk.</p>
<p>Converts a dask bag holding data from a CBS table to Parquet form
and stores it on disk. The bag should be filled by dicts (can be nested)
which can be serialized as json.</p>
<p>The current implementation iterates over each bag partition and dumps
it into a json file, then appends all file into a single json file. That
json file is then read into a PyArrow table, and finally that table is
written as a parquet file to disk.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>bag</code></strong> :&ensp;<code>Dask Bag</code></dt>
<dd>A Bag holding (possibly nested) dicts that can serialized as json</dd>
<dt><strong><code>file_name</code></strong> :&ensp;<code>str)</code></dt>
<dd>The name of the file to store on disk</dd>
<dt><strong><code>out_dir</code></strong> :&ensp;<code>str</code> or <code>Path</code></dt>
<dd>A path to the directory where the file is stored</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>pq_path</code></strong> :&ensp;<code>Path</code></dt>
<dd>The path to the output parquet file</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def convert_table_to_parquet(
    bag, file_name: str, out_dir: Union[str, Path]
) -&gt; Path:  # (TODO -&gt; IS THERE A FASTER/BETTER WAY??)
    &#34;&#34;&#34;Converts a Dask Bag to Parquet files and stores them on disk.

    Converts a dask bag holding data from a CBS table to Parquet form
    and stores it on disk. The bag should be filled by dicts (can be nested)
    which can be serialized as json.

    The current implementation iterates over each bag partition and dumps
    it into a json file, then appends all file into a single json file. That
    json file is then read into a PyArrow table, and finally that table is
    written as a parquet file to disk.

    Parameters
    ----------
    bag: Dask Bag
        A Bag holding (possibly nested) dicts that can serialized as json
    file_name: str)
        The name of the file to store on disk
    out_dir: str or Path
        A path to the directory where the file is stored

    Returns
    -------
    pq_path: Path
        The path to the output parquet file
    &#34;&#34;&#34;

    # create directories to store files
    out_dir = Path(out_dir)
    temp_json_dir = Path(&#34;./temp/json&#34;)
    create_dir(temp_json_dir)
    create_dir(out_dir)

    # File path to dump table as ndjson
    json_path = Path(f&#34;{temp_json_dir}/{file_name}.json&#34;)
    # File path to create as parquet file
    pq_path = Path(f&#34;{out_dir}/{file_name}.parquet&#34;)

    # Dump each bag partition to json file
    bag.map(json.dumps).to_textfiles(temp_json_dir / &#34;*.json&#34;)
    # Get all json file names with path
    filenames = sorted(glob(str(temp_json_dir) + &#34;/*.json&#34;))
    # Append all jsons into a single file  ## Also possible to use Dask Delayed here https://stackoverflow.com/questions/39566809/writing-dask-partitions-into-single-file
    with open(json_path, &#34;w+&#34;) as json_file:
        for fn in filenames:
            with open(fn) as f:
                json_file.write(f.read())
            remove(fn)

    # # Works without converting to ndjson - might be needed in a different implementation?
    # # Convert to ndjson format
    # with open(json_path, &#39;w+&#39;) as ndjson:
    #     for record in table:
    #         ndjson.write(json.dumps(record) + &#34;\n&#34;)

    # Create PyArrow table from ndjson file
    pa_table = pa_json.read_json(json_path)

    # Store parquet table #TODO -&gt; set proper data types in parquet file
    pq.write_table(pa_table, pq_path)

    # Remove temp ndjson file
    remove(json_path)
    # Remove temp folder if empty  #TODO -&gt; inefficiently(?) creates and removes the folder each time the function is called
    if not listdir(temp_json_dir):
        rmdir(temp_json_dir)
    return pq_path</code></pre>
</details>
</dd>
<dt id="statline_bq.utils.create_bq_dataset"><code class="name flex">
<span>def <span class="ident">create_bq_dataset</span></span>(<span>id: str, source: str = 'cbs', odata_version: str = None, description: str = None, gcp: <a title="statline_bq.config.Gcp" href="config.html#statline_bq.config.Gcp">Gcp</a> = None) ‑> str</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a dataset in Google Big Query. If dataset exists already exists, does nothing.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>id</code></strong> :&ensp;<code>str</code></dt>
<dd>CBS Dataset id, i.e. "83583NED".</dd>
<dt><strong><code>source</code></strong> :&ensp;<code>str</code>, default=<code>"cbs"</code></dt>
<dd>The source of the dataset. Currently only "cbs" is relevant.</dd>
<dt><strong><code>odata_version</code></strong> :&ensp;<code>str</code></dt>
<dd>version of the odata for this dataset - must be either "v3" or "v4".</dd>
<dt><strong><code>description</code></strong> :&ensp;<code>str</code></dt>
<dd>The description of the dataset</dd>
<dt><strong><code>gcp</code></strong> :&ensp;<code>Gcp</code></dt>
<dd>A Gcp Class object, holding GCP parameters</dd>
</dl>
<p>Returns:
dataset.dataset_id: str
The id of the created BQ dataset</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_bq_dataset(
    id: str,
    source: str = &#34;cbs&#34;,
    odata_version: str = None,
    description: str = None,
    gcp: Gcp = None,
) -&gt; str:
    &#34;&#34;&#34;Creates a dataset in Google Big Query. If dataset exists already exists, does nothing.

    Parameters
    ----------
    id: str
        CBS Dataset id, i.e. &#34;83583NED&#34;.
    source: str, default=&#34;cbs&#34;
        The source of the dataset. Currently only &#34;cbs&#34; is relevant.
    odata_version: str
        version of the odata for this dataset - must be either &#34;v3&#34; or &#34;v4&#34;.
    description: str
        The description of the dataset
    gcp: Gcp
        A Gcp Class object, holding GCP parameters

    Returns:
    dataset.dataset_id: str
        The id of the created BQ dataset
    &#34;&#34;&#34;

    # Construct a BigQuery client object.
    client = bigquery.Client(project=gcp.dev.project_id)

    # Set dataset_id to the ID of the dataset to create.
    dataset_id = f&#34;{client.project}.{source}_{odata_version}_{id}&#34;

    # Construct a full Dataset object to send to the API.
    dataset = bigquery.Dataset(dataset_id)

    # Specify the geographic location where the dataset should reside.
    dataset.location = gcp.dev.location

    # Add description if provided
    dataset.description = description

    # Send the dataset to the API for creation, with an explicit timeout.
    # Raises google.api_core.exceptions.Conflict if the Dataset already
    # exists within the project.
    try:
        dataset = client.create_dataset(dataset, timeout=30)  # Make an API request.
        print(f&#34;Created dataset {client.project}.{dataset.dataset_id}&#34;)
    except exceptions.Conflict:
        print(f&#34;Dataset {client.project}.{dataset.dataset_id} already exists&#34;)
    finally:
        return dataset.dataset_id</code></pre>
</details>
</dd>
<dt id="statline_bq.utils.create_dir"><code class="name flex">
<span>def <span class="ident">create_dir</span></span>(<span>path: pathlib.Path) ‑> pathlib.Path</span>
</code></dt>
<dd>
<div class="desc"><p>Checks whether a path exists and is a directory, and creates it if not.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>path</code></strong> :&ensp;<code>Path</code></dt>
<dd>A path to the directory.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>path</code></strong> :&ensp;<code>Path</code></dt>
<dd>The same input path, to new or existing directory.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_dir(path: Path) -&gt; Path:
    &#34;&#34;&#34;Checks whether a path exists and is a directory, and creates it if not.

    Parameters
    ----------
    path: Path
        A path to the directory.

    Returns
    -------
    path: Path
        The same input path, to new or existing directory.
    &#34;&#34;&#34;

    try:
        path = Path(path)
        if not (path.exists() and path.is_dir()):
            path.mkdir(parents=True)
        return path
    except TypeError as error:
        print(f&#34;Error trying to find {path}: {error!s}&#34;)
        return None</code></pre>
</details>
</dd>
<dt id="statline_bq.utils.create_named_dir"><code class="name flex">
<span>def <span class="ident">create_named_dir</span></span>(<span>id: str, odata_version: str, source: str = 'cbs', config: <a title="statline_bq.config.Config" href="config.html#statline_bq.config.Config">Config</a> = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a directory according to a specific structure.</p>
<p>A convenience function, creatind a directory according to the following
pattern, based on a config object and the rest of the parameters. Meant to
create a directory for each dataset where its related tables are written
into as parquet files.</p>
<p>Directory pattern:</p>
<pre><code>"~/{config.paths.root}/{config.paths.temp}/{source}/{id}/{date_of_creation}/parquet"
</code></pre>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>id</code></strong> :&ensp;<code>str</code></dt>
<dd>CBS Dataset id, i.e. "83583NED"</dd>
<dt><strong><code>odata_version</code></strong> :&ensp;<code>str</code></dt>
<dd>version of the odata for this dataset - must be either "v3" or "v4".</dd>
<dt><strong><code>source</code></strong> :&ensp;<code>str</code>, default=<code>"cbs"</code></dt>
<dd>The source of the dataset. Currently only "cbs" is relevant.</dd>
<dt><strong><code>config</code></strong> :&ensp;<code>Config object</code></dt>
<dd>Config object holding GCP and local paths</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>path_to_named_dir</code></strong> :&ensp;<code>Path</code></dt>
<dd>path to created folder</dd>
</dl>
<hr>
<h2 id="example">Example</h2>
<pre><code class="language-python">&gt;&gt;&gt; from statline_bq.utils import create_named_dir
&gt;&gt;&gt; from statline_bq.config import get_config
&gt;&gt;&gt; id = &quot;83583NED&quot;
&gt;&gt;&gt; config = get_config(&quot;path/to/config.file&quot;)
&gt;&gt;&gt; print(config.paths.root)
Projects/statline-bq
&gt;&gt;&gt; print(config.paths.temp)
temp
&gt;&gt;&gt; dir = create_named_dir(id=id, odata_version=&quot;v3&quot;)
&gt;&gt;&gt; dir
PosixPath('/Users/{YOUR_USERNAME}/Projects/statline-bq/temp/cbs/v3/83583NED/20201214/parquet')
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_named_dir(
    id: str, odata_version: str, source: str = &#34;cbs&#34;, config: Config = None
):
    &#34;&#34;&#34;Creates a directory according to a specific structure.

    A convenience function, creatind a directory according to the following
    pattern, based on a config object and the rest of the parameters. Meant to
    create a directory for each dataset where its related tables are written
    into as parquet files.

    Directory pattern:
        
        &#34;~/{config.paths.root}/{config.paths.temp}/{source}/{id}/{date_of_creation}/parquet&#34;

    Parameters
    ----------
    id: str
        CBS Dataset id, i.e. &#34;83583NED&#34;
    odata_version: str
        version of the odata for this dataset - must be either &#34;v3&#34; or &#34;v4&#34;.
    source: str, default=&#34;cbs&#34;
        The source of the dataset. Currently only &#34;cbs&#34; is relevant.
    config: Config object
        Config object holding GCP and local paths

    Returns
    -------
    path_to_named_dir: Path
        path to created folder

    -------
    Example
    -------
    &gt;&gt;&gt; from statline_bq.utils import create_named_dir
    &gt;&gt;&gt; from statline_bq.config import get_config
    &gt;&gt;&gt; id = &#34;83583NED&#34;
    &gt;&gt;&gt; config = get_config(&#34;path/to/config.file&#34;)
    &gt;&gt;&gt; print(config.paths.root)
    Projects/statline-bq
    &gt;&gt;&gt; print(config.paths.temp)
    temp
    &gt;&gt;&gt; dir = create_named_dir(id=id, odata_version=&#34;v3&#34;)
    &gt;&gt;&gt; dir
    PosixPath(&#39;/Users/{YOUR_USERNAME}/Projects/statline-bq/temp/cbs/v3/83583NED/20201214/parquet&#39;)
    &#34;&#34;&#34;

    # Get paths from config object
    root = Path.home() / Path(config.paths.root)
    temp = root / Path(config.paths.temp)

    # Create placeholders for storage
    path = temp / Path(
        f&#34;{source}/{odata_version}/{id}/{datetime.today().date().strftime(&#39;%Y%m%d&#39;)}/parquet&#34;
    )
    path_to_named_dir = create_dir(path)
    return path_to_named_dir</code></pre>
</details>
</dd>
<dt id="statline_bq.utils.delete_bq_dataset"><code class="name flex">
<span>def <span class="ident">delete_bq_dataset</span></span>(<span>id: str, source: str = 'cbs', odata_version: str = None, gcp: <a title="statline_bq.config.Gcp" href="config.html#statline_bq.config.Gcp">Gcp</a> = None) ‑> NoneType</span>
</code></dt>
<dd>
<div class="desc"><p>Delete an exisiting dataset from Google Big Query</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>id</code></strong> :&ensp;<code>str</code></dt>
<dd>CBS Dataset id, i.e. "83583NED".</dd>
<dt><strong><code>source</code></strong> :&ensp;<code>str</code>, default=<code>"cbs"</code></dt>
<dd>The source of the dataset. Currently only "cbs" is relevant.</dd>
<dt><strong><code>odata_version</code></strong> :&ensp;<code>str</code></dt>
<dd>version of the odata for this dataset - must be either "v3" or "v4".</dd>
<dt><strong><code>gcp</code></strong> :&ensp;<code>Gcp</code></dt>
<dd>A Gcp Class object, holding GCP parameters</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def delete_bq_dataset(
    id: str, source: str = &#34;cbs&#34;, odata_version: str = None, gcp: Gcp = None
) -&gt; None:
    &#34;&#34;&#34;Delete an exisiting dataset from Google Big Query

    Parameters
    ----------
    id: str
        CBS Dataset id, i.e. &#34;83583NED&#34;.
    source: str, default=&#34;cbs&#34;
        The source of the dataset. Currently only &#34;cbs&#34; is relevant.
    odata_version: str
        version of the odata for this dataset - must be either &#34;v3&#34; or &#34;v4&#34;.
    gcp: Gcp
        A Gcp Class object, holding GCP parameters

    Returns
    -------
    None
    &#34;&#34;&#34;

    # Construct a bq client
    client = bigquery.Client(project=gcp.dev.project_id)

    # Set bq dataset id string
    dataset_id = f&#34;{source}_{odata_version}_{id}&#34;

    # Delete the dataset and its contents
    client.delete_dataset(dataset_id, delete_contents=True, not_found_ok=True)

    return None</code></pre>
</details>
</dd>
<dt id="statline_bq.utils.gcs_to_gbq"><code class="name flex">
<span>def <span class="ident">gcs_to_gbq</span></span>(<span>id: str, source: str = 'cbs', odata_version: str = None, third_party: bool = False, config: <a title="statline_bq.config.Config" href="config.html#statline_bq.config.Config">Config</a> = None, gcs_folder: str = None, file_names: list = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Creates a BQ dataset and links all relevant tables from GCS underneath.</p>
<p>Creates a dataset (if does not exist) in Google Big Query, and underneath
creates permanent tables linked to parquet file stored in Google Storage.
If dataset exists, removes it and recreates it with most up to date uploaded files (?) # TODO: Is this the best logic?</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>id</code></strong> :&ensp;<code>str</code></dt>
<dd>CBS Dataset id, i.e. "83583NED".</dd>
<dt><strong><code>source</code></strong> :&ensp;<code>str</code>, default=<code>"cbs"</code></dt>
<dd>The source of the dataset. Currently only "cbs" is relevant.</dd>
<dt><strong><code>odata_version</code></strong> :&ensp;<code>str</code></dt>
<dd>version of the odata for this dataset - must be either "v3" or "v4".</dd>
<dt>third_party : bool, default=False</dt>
<dt>Flag to indicate dataset is not originally from CBS. Set to true to use dataderden.cbs.nl as base url (not available in v4 yet).</dt>
<dt>config : Config object</dt>
<dt>Config object holding GCP and local paths.</dt>
<dt><strong><code>gcs_folder</code></strong> :&ensp;<code>str</code></dt>
<dd>The GCS folder holding the description txt file.</dd>
<dt><strong><code>file_names</code></strong> :&ensp;<code>list</code></dt>
<dd>A list holding all file names of tables to be linked.</dd>
</dl>
<h2 id="returns">Returns</h2>
<h1 id="todo-return-job-id">TODO Return job id</h1>
<pre><code>[description]
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def gcs_to_gbq(
    id: str,
    source: str = &#34;cbs&#34;,
    odata_version: str = None,
    third_party: bool = False,
    config: Config = None,
    gcs_folder: str = None,
    file_names: list = None,
):
    &#34;&#34;&#34;Creates a BQ dataset and links all relevant tables from GCS underneath.
    
    Creates a dataset (if does not exist) in Google Big Query, and underneath
    creates permanent tables linked to parquet file stored in Google Storage.
    If dataset exists, removes it and recreates it with most up to date uploaded files (?) # TODO: Is this the best logic?
    
    Parameters
    ----------
    id: str
        CBS Dataset id, i.e. &#34;83583NED&#34;.
    source: str, default=&#34;cbs&#34;
        The source of the dataset. Currently only &#34;cbs&#34; is relevant.
    odata_version: str
        version of the odata for this dataset - must be either &#34;v3&#34; or &#34;v4&#34;.
    third_party : bool, default=False
        Flag to indicate dataset is not originally from CBS. Set to true to use dataderden.cbs.nl as base url (not available in v4 yet).
    config : Config object
        Config object holding GCP and local paths.
    gcs_folder : str
        The GCS folder holding the description txt file.
    file_names : list
        A list holding all file names of tables to be linked.


    Returns
    -------
    # TODO Return job id
        [description]
    &#34;&#34;&#34;

    # # Get all parquet files in gcs folder from GCS
    # storage_client = storage.Client(project=gcp.dev.project_id)

    # TODO: retrieve names from GCS? If yes, loop below should change to use these two lists
    # blob_uris = [
    #     blob.self_link
    #     for blob in storage_client.list_blobs(gcp.dev.bucket, prefix=gcs_folder)
    #     if not blob.name.endswith(&#34;.txt&#34;)
    # ]
    # blob_names = [
    #     blob.name
    #     for blob in storage_client.list_blobs(gcp.dev.bucket, prefix=gcs_folder)
    #     if not blob.name.endswith(&#34;.txt&#34;)
    # ]

    # Get description text from txt file
    description = get_description_from_gcs(
        id=id,
        source=source,
        odata_version=odata_version,
        gcp=config.gcp,
        gcs_folder=gcs_folder,
    )

    # Check if dataset exists and delete if it does TODO: maybe delete anyway (deleting uses not_found_ok to ignore error if does not exist)
    if check_bq_dataset(
        id=id, source=source, odata_version=odata_version, gcp=config.gcp
    ):
        delete_bq_dataset(
            id=id, source=source, odata_version=odata_version, gcp=config.gcp
        )

    # Create a dataset in BQ
    dataset_id = create_bq_dataset(
        id=id,
        source=source,
        odata_version=odata_version,
        description=description,
        gcp=config.gcp,
    )
    # if not existing:
    # Skip?
    # else:
    # Handle existing dataset - delete and recreate? Repopulate? TODO

    # Initialize client
    client = bigquery.Client(project=config.gcp.dev.project_id)

    # Configure the external data source
    # dataset_id = f&#34;{source}_{odata_version}_{id}&#34;
    dataset_ref = bigquery.DatasetReference(config.gcp.dev.project_id, dataset_id)

    # Loop over all files related to this dataset id
    for name in file_names:
        # Configure the external data source per table id
        table_id = str(name).split(&#34;.&#34;)[2]
        table = bigquery.Table(dataset_ref.table(table_id))

        external_config = bigquery.ExternalConfig(&#34;PARQUET&#34;)
        external_config.source_uris = [
            f&#34;https://storage.cloud.google.com/{config.gcp.dev.bucket}/{gcs_folder}/{name}&#34;  # TODO: Handle dev/test/prod?
        ]
        table.external_data_configuration = external_config
        # table.description = description

        # Create a permanent table linked to the GCS file
        table = client.create_table(
            table, exists_ok=True
        )  # BUG: error raised, using exists_ok=True to avoid
    return None  # TODO Return job id</code></pre>
</details>
</dd>
<dt id="statline_bq.utils.get_dataset_description"><code class="name flex">
<span>def <span class="ident">get_dataset_description</span></span>(<span>urls: dict, odata_version: str) ‑> str</span>
</code></dt>
<dd>
<div class="desc"><p>Gets a CBS dataset description text.</p>
<p>Wrapper function to call the correct version function which in turn gets
the dataset description according to the odata version: "v3" or "v4".</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>urls</code></strong> :&ensp;<code>dict</code></dt>
<dd>Dictionary holding urls of the dataset from CBS.
NOTE: urls["Properties"] (for v4) or urls["TableInfos"] (for v3)
must be present in order to access the dataset description.</dd>
<dt><strong><code>odata_version</code></strong> :&ensp;<code>str</code></dt>
<dd>version of the odata for this dataset - must be either "v3" or "v4".</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>description</code></strong> :&ensp;<code>str</code></dt>
<dd>The description of the dataset from CBS.</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code class="language-python">&gt;&gt;&gt; from statline_bq.utils import get_dataset_description
&gt;&gt;&gt; urls = {
...         &quot;TableInfos&quot;: &quot;https://opendata.cbs.nl/ODataFeed/odata/83583NED/TableInfos&quot;,  
...         &quot;UntypedDataSet&quot;: &quot;https://opendata.cbs.nl/ODataFeed/odata/83583NED/UntypedDataSet&quot;
...         }
&gt;&gt;&gt; odata_version = &quot;v3&quot;
&gt;&gt;&gt; description_text = get_dataset_description(urls, odata_version=odata_version)
&gt;&gt;&gt; description_text
#Text describing the dataset will print
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_dataset_description(urls: dict, odata_version: str) -&gt; str:
    &#34;&#34;&#34;Gets a CBS dataset description text.
    
    Wrapper function to call the correct version function which in turn gets
    the dataset description according to the odata version: &#34;v3&#34; or &#34;v4&#34;.

    Parameters
    ----------
    urls: dict
        Dictionary holding urls of the dataset from CBS.
        NOTE: urls[&#34;Properties&#34;] (for v4) or urls[&#34;TableInfos&#34;] (for v3)
        must be present in order to access the dataset description.

    odata_version: str
        version of the odata for this dataset - must be either &#34;v3&#34; or &#34;v4&#34;.

    Returns
    -------
    description: str
        The description of the dataset from CBS.

    Examples
    --------
    &gt;&gt;&gt; from statline_bq.utils import get_dataset_description
    &gt;&gt;&gt; urls = {
    ...         &#34;TableInfos&#34;: &#34;https://opendata.cbs.nl/ODataFeed/odata/83583NED/TableInfos&#34;,  
    ...         &#34;UntypedDataSet&#34;: &#34;https://opendata.cbs.nl/ODataFeed/odata/83583NED/UntypedDataSet&#34;
    ...         }
    &gt;&gt;&gt; odata_version = &#34;v3&#34;
    &gt;&gt;&gt; description_text = get_dataset_description(urls, odata_version=odata_version)
    &gt;&gt;&gt; description_text
    #Text describing the dataset will print
    &#34;&#34;&#34;

    if odata_version.lower() == &#34;v4&#34;:
        description = get_dataset_description_v4(urls[&#34;Properties&#34;])
    elif odata_version.lower() == &#34;v3&#34;:
        description = get_dataset_description_v3(urls[&#34;TableInfos&#34;])
    else:
        raise ValueError(&#34;odata version must be either &#39;v3&#39; or &#39;v4&#39;&#34;)
    return description</code></pre>
</details>
</dd>
<dt id="statline_bq.utils.get_dataset_description_v3"><code class="name flex">
<span>def <span class="ident">get_dataset_description_v3</span></span>(<span>url_table_infos: str) ‑> str</span>
</code></dt>
<dd>
<div class="desc"><p>Gets the description of a v3 odata dataset from CBS provided in url_table_infos.</p>
<p>Usually wrapped in <code><a title="statline_bq.utils.get_dataset_description" href="#statline_bq.utils.get_dataset_description">get_dataset_description()</a></code>, and it is better practice
to call it wrapped to allow for both "v3" and "v4" functionality.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>url_table_infos</code></strong> :&ensp;<code>str</code></dt>
<dd>The url for a dataset's "TableInfos" table as string.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>description</code></strong> :&ensp;<code>str</code></dt>
<dd>A string with the dataset's description</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_dataset_description_v3(url_table_infos: str) -&gt; str:
    &#34;&#34;&#34;Gets the description of a v3 odata dataset from CBS provided in url_table_infos.

    Usually wrapped in `get_dataset_description()`, and it is better practice
    to call it wrapped to allow for both &#34;v3&#34; and &#34;v4&#34; functionality.

    Parameters
    ----------
    url_table_infos: str
        The url for a dataset&#39;s &#34;TableInfos&#34; table as string.

    Returns
    -------
    description: str
        A string with the dataset&#39;s description
    &#34;&#34;&#34;

    # Get JSON format of data set.
    url_table_infos = &#34;?&#34;.join((url_table_infos, &#34;$format=json&#34;))

    data_info = requests.get(url_table_infos).json()  # Is of type dict()

    data_info_values = data_info[&#34;value&#34;]  # Is of type list

    # Get short description as text
    description = data_info_values[0][&#34;ShortDescription&#34;]

    return description</code></pre>
</details>
</dd>
<dt id="statline_bq.utils.get_dataset_description_v4"><code class="name flex">
<span>def <span class="ident">get_dataset_description_v4</span></span>(<span>url_table_properties: str) ‑> str</span>
</code></dt>
<dd>
<div class="desc"><p>Gets table description of a table in CBS odata V4.</p>
<p>Usually wrapped in <code><a title="statline_bq.utils.get_dataset_description" href="#statline_bq.utils.get_dataset_description">get_dataset_description()</a></code>, and it is better practice
to call it wrapped to allow for both "v3" and "v4" functionality.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>url_table_properties</code></strong> :&ensp;<code>str</code></dt>
<dd>The url for a dataset's "Properties" table as string.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>description</code></strong> :&ensp;<code>str</code></dt>
<dd>A string with the dataset's description</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_dataset_description_v4(url_table_properties: str) -&gt; str:
    &#34;&#34;&#34;Gets table description of a table in CBS odata V4.

    Usually wrapped in `get_dataset_description()`, and it is better practice
    to call it wrapped to allow for both &#34;v3&#34; and &#34;v4&#34; functionality.

    Parameters
    ----------
    url_table_properties: str
        The url for a dataset&#39;s &#34;Properties&#34; table as string.

    Returns
    -------
    description: str
        A string with the dataset&#39;s description
    &#34;&#34;&#34;

    r = requests.get(url_table_properties).json()
    return r[&#34;Description&#34;]</code></pre>
</details>
</dd>
<dt id="statline_bq.utils.get_description_from_gcs"><code class="name flex">
<span>def <span class="ident">get_description_from_gcs</span></span>(<span>id: str, source: str = 'cbs', odata_version: str = None, gcp: <a title="statline_bq.config.Gcp" href="config.html#statline_bq.config.Gcp">Gcp</a> = None, gcs_folder: str = None) ‑> str</span>
</code></dt>
<dd>
<div class="desc"><p>Gets previsouly uploaded dataset description from GCS.</p>
<p>The description should exist in the following file, under the following structure:</p>
<pre><code>"{project}/{bucket}/{source}/{odata_version}/{id}/{YYYYMMDD}/{source}.{odata_version}.{id}_Description"
</code></pre>
<p>For example:</p>
<pre><code>"dataverbinders-dev/cbs/v4/83765NED/20201127/cbs.v4.83765NED_Description.txt"
</code></pre>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>id</code></strong> :&ensp;<code>str</code></dt>
<dd>CBS Dataset id, i.e. "83583NED".</dd>
<dt><strong><code>source</code></strong> :&ensp;<code>str</code>, default=<code>"cbs"</code></dt>
<dd>The source of the dataset. Currently only "cbs" is relevant.</dd>
<dt><strong><code>odata_version</code></strong> :&ensp;<code>str</code></dt>
<dd>version of the odata for this dataset - must be either "v3" or "v4".</dd>
<dt><strong><code>gcp</code></strong> :&ensp;<code>Gcp</code></dt>
<dd>A Gcp Class object, holding GCP parameters</dd>
<dt><strong><code>gcs_folder</code></strong> :&ensp;<code>str</code></dt>
<dd>The GCS folder holding the description txt file</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>str</code></dt>
<dd>[description]</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_description_from_gcs(
    id: str,
    source: str = &#34;cbs&#34;,
    odata_version: str = None,
    gcp: Gcp = None,
    gcs_folder: str = None,
) -&gt; str:
    &#34;&#34;&#34;Gets previsouly uploaded dataset description from GCS.

    The description should exist in the following file, under the following structure:

        &#34;{project}/{bucket}/{source}/{odata_version}/{id}/{YYYYMMDD}/{source}.{odata_version}.{id}_Description&#34;

    For example:

        &#34;dataverbinders-dev/cbs/v4/83765NED/20201127/cbs.v4.83765NED_Description.txt&#34;

    Parameters
    ----------
    id: str
        CBS Dataset id, i.e. &#34;83583NED&#34;.
    source: str, default=&#34;cbs&#34;
        The source of the dataset. Currently only &#34;cbs&#34; is relevant.
    odata_version: str
        version of the odata for this dataset - must be either &#34;v3&#34; or &#34;v4&#34;.
    gcp: Gcp
        A Gcp Class object, holding GCP parameters
    gcs_folder : str
        The GCS folder holding the description txt file

    Returns
    -------
    str
        [description]
    &#34;&#34;&#34;

    client = storage.Client(project=gcp.dev.project_id)
    bucket = client.get_bucket(gcp.dev.bucket)
    blob = bucket.get_blob(
        f&#34;{gcs_folder}/{source}.{odata_version}.{id}_Description.txt&#34;
    )
    return str(blob.download_as_string().decode(&#34;utf-8&#34;))</code></pre>
</details>
</dd>
<dt id="statline_bq.utils.get_file_names"><code class="name flex">
<span>def <span class="ident">get_file_names</span></span>(<span>paths: Iterable[Union[str, os.PathLike]]) ‑> list</span>
</code></dt>
<dd>
<div class="desc"><p>Gets the filenames from an iterable of Path-like objects</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>paths</code></strong> :&ensp;<code>iterable</code> of <code>strings</code> or <code>path-like objects</code></dt>
<dd>An iterable holding path-strings or path-like objects</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>file_names</code></strong> :&ensp;<code>list</code> of <code>str</code></dt>
<dd>A list holding the extracted file names</dd>
</dl>
<h2 id="example">Example</h2>
<pre><code class="language-python">&gt;&gt;&gt; from pathlib import Path
</code></pre>
<pre><code class="language-python">&gt;&gt;&gt; path1 = Path('some_folder/other_folder/some_file.txt')
&gt;&gt;&gt; path2 = 'some_folder/different_folder/another_file.png'
&gt;&gt;&gt; full_paths = [path1, path2]
</code></pre>
<pre><code class="language-python">&gt;&gt;&gt; file_names = get_file_names(full_paths)
</code></pre>
<pre><code class="language-python">&gt;&gt;&gt; for name in file_names:
        print(name)
some_file.txt
another_file.png
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_file_names(paths: Iterable[Union[str, PathLike]]) -&gt; list:
    &#34;&#34;&#34;Gets the filenames from an iterable of Path-like objects

    Parameters
    ----------
    paths: iterable of strings or path-like objects
        An iterable holding path-strings or path-like objects

    Returns
    -------
    file_names: list of str
        A list holding the extracted file names

    Example
    -------
    &gt;&gt;&gt; from pathlib import Path

    &gt;&gt;&gt; path1 = Path(&#39;some_folder/other_folder/some_file.txt&#39;)
    &gt;&gt;&gt; path2 = &#39;some_folder/different_folder/another_file.png&#39;
    &gt;&gt;&gt; full_paths = [path1, path2]

    &gt;&gt;&gt; file_names = get_file_names(full_paths)

    &gt;&gt;&gt; for name in file_names:
            print(name)
    some_file.txt
    another_file.png
    &#34;&#34;&#34;

    paths = [Path(path) for path in paths]
    file_names = [path.name for path in paths]
    return file_names</code></pre>
</details>
</dd>
<dt id="statline_bq.utils.get_odata"><code class="name flex">
<span>def <span class="ident">get_odata</span></span>(<span>target_url: str, odata_version: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Gets a table from a valid CBS url and returns it as a Dask bag.</p>
<p>A wrapper, calling the appropriate version function to get the table from
a valid CBS url, leading to a table in OData format.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>target_url</code></strong> :&ensp;<code>str</code></dt>
<dd>A url to the table</dd>
<dt><strong><code>odata_version</code></strong> :&ensp;<code>str</code></dt>
<dd>version of the odata for this dataset - must be either "v3" or "v4".</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Dask Bag</code></dt>
<dd>All data received from target url as json type, concatenated in a Dask Bag</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If 'odata_version` is not one of {"v3", "v4"}</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_odata(target_url: str, odata_version: str):
    &#34;&#34;&#34;Gets a table from a valid CBS url and returns it as a Dask bag.

    A wrapper, calling the appropriate version function to get the table from
    a valid CBS url, leading to a table in OData format.

    Parameters
    ----------
    target_url : str
        A url to the table
    odata_version : str
        version of the odata for this dataset - must be either &#34;v3&#34; or &#34;v4&#34;.

    Returns
    -------
    Dask Bag
        All data received from target url as json type, concatenated in a Dask Bag

    Raises
    ------
    ValueError
        If &#39;odata_version` is not one of {&#34;v3&#34;, &#34;v4&#34;}
    &#34;&#34;&#34;

    if odata_version == &#34;v4&#34;:
        return get_odata_v4(target_url)
    elif odata_version == &#34;v3&#34;:
        return get_odata_v3(target_url)
    else:
        raise ValueError(&#34;odata version must be either &#39;v3&#39; or &#39;v4&#39;&#34;)</code></pre>
</details>
</dd>
<dt id="statline_bq.utils.get_odata_v3"><code class="name flex">
<span>def <span class="ident">get_odata_v3</span></span>(<span>target_url: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Gets a table from a valid url for CBS dataset with Odata v3.</p>
<p>This function uses standard requests.get() to retrieve data at target_url
in json format, and concats it all into a Dask Bag to handle memory
overflow if needed.</p>
<p>Each request from CBS is limited to 10,000 rows, and if more data exists
the key "odata.nextLink" exists in the response with the link to the next
10,000 (or less) rows.</p>
<p>Meant to be wrapped by <code><a title="statline_bq.utils.get_odata" href="#statline_bq.utils.get_odata">get_odata()</a></code>, and it is better practice to call it
wrapped to allow for both "v3" and "v4" functionality.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>target_url</code></strong> :&ensp;<code>str</code></dt>
<dd>A valid url of a table from CBS</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>bag</code></strong> :&ensp;<code>Dask Bag</code></dt>
<dd>All data received from target url as json type, concatenated as a Dask bag</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_odata_v3(target_url: str):
    &#34;&#34;&#34;Gets a table from a valid url for CBS dataset with Odata v3.

    This function uses standard requests.get() to retrieve data at target_url
    in json format, and concats it all into a Dask Bag to handle memory
    overflow if needed.

    Each request from CBS is limited to 10,000 rows, and if more data exists
    the key &#34;odata.nextLink&#34; exists in the response with the link to the next
    10,000 (or less) rows.

    Meant to be wrapped by `get_odata()`, and it is better practice to call it
    wrapped to allow for both &#34;v3&#34; and &#34;v4&#34; functionality.

    Parameters
    ----------
    target_url: str
        A valid url of a table from CBS

    Returns
    -------
    bag: Dask Bag
        All data received from target url as json type, concatenated as a Dask bag
    &#34;&#34;&#34;

    print(f&#34;Fetching from {target_url}&#34;)
    # First call target url and get json formatted response as dict
    r = requests.get(target_url).json()

    # Initialize bag as None
    bag = None

    # Create Dask bag from dict (check if not empty field)
    if r[&#34;value&#34;]:
        bag = db.from_sequence(r[&#34;value&#34;])  # TODO -&gt; define npartitions?

    # check if more data exists
    if &#34;odata.nextLink&#34; in r:
        target_url = r[&#34;odata.nextLink&#34;]
    else:
        target_url = None

    # if more data exists continue to concat bag until complete
    while target_url:
        r = requests.get(target_url).json()
        if r[&#34;value&#34;]:
            temp_bag = db.from_sequence(r[&#34;value&#34;])
            bag = db.concat([bag, temp_bag])

        if &#34;odata.nextLink&#34; in r:
            target_url = r[&#34;odata.nextLink&#34;]
        else:
            target_url = None

    return bag</code></pre>
</details>
</dd>
<dt id="statline_bq.utils.get_odata_v4"><code class="name flex">
<span>def <span class="ident">get_odata_v4</span></span>(<span>target_url: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Gets a table from a specific url for CBS Odata v4.
This function uses standard requests.get() to retrieve data at target_url
in json format, and concats it all into a Dask Bag to handle memory
overflow if needed.</p>
<p>Each request from CBS is limited to 10,000 rows, and if more data exists
the key "@odata.nextLink" exists in the response with the link to the next
10,000 (or less) rows.</p>
<p>Meant to be wrapped by <code><a title="statline_bq.utils.get_odata" href="#statline_bq.utils.get_odata">get_odata()</a></code>, and it is better practice to call it
wrapped to allow for both "v3" and "v4" functionality.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>target_url</code></strong> :&ensp;<code>str</code></dt>
<dd>A valid url of a table from CBS</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>bag</code></strong> :&ensp;<code>Dask Bag</code></dt>
<dd>All data received from target url as json type, concatenated as a Dask bag</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_odata_v4(
    target_url: str,
):  # TODO -&gt; How to define Bag for type hinting? (maybe here: https://docs.python.org/3/library/typing.html#newtype)
    &#34;&#34;&#34;Gets a table from a specific url for CBS Odata v4.
    This function uses standard requests.get() to retrieve data at target_url
    in json format, and concats it all into a Dask Bag to handle memory
    overflow if needed.

    Each request from CBS is limited to 10,000 rows, and if more data exists
    the key &#34;@odata.nextLink&#34; exists in the response with the link to the next
    10,000 (or less) rows.

    Meant to be wrapped by `get_odata()`, and it is better practice to call it
    wrapped to allow for both &#34;v3&#34; and &#34;v4&#34; functionality.

    Parameters
    ----------
    target_url: str
        A valid url of a table from CBS

    Returns
    -------
    bag: Dask Bag
        All data received from target url as json type, concatenated as a Dask bag
    &#34;&#34;&#34;

    print(f&#34;Fetching from {target_url}&#34;)
    # First call target url and get json formatted response as dict
    r = requests.get(target_url).json()
    # Create Dask bag from dict
    bag = db.from_sequence(r[&#34;value&#34;])  # TODO -&gt; define npartitions?

    # check if more data exists
    if &#34;@odata.nextLink&#34; in r:
        target_url = r[&#34;@odata.nextLink&#34;]
    else:
        target_url = None

    # if more data exists continue to concat bag until complete
    while target_url:
        r = requests.get(target_url).json()
        temp_bag = db.from_sequence(r[&#34;value&#34;])
        bag = db.concat([bag, temp_bag])

        if &#34;@odata.nextLink&#34; in r:
            target_url = r[&#34;@odata.nextLink&#34;]
        else:
            target_url = None

    return bag</code></pre>
</details>
</dd>
<dt id="statline_bq.utils.get_odata_v4_curl"><code class="name flex">
<span>def <span class="ident">get_odata_v4_curl</span></span>(<span>target_url: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Retrieves a table from a specific url for CBS Odata v4.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>url_table_properties</code></strong> :&ensp;<code>str</code></dt>
<dd>The url of the desired table</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>Dask bag</code></dt>
<dd>All data received from target url as json type, in a Dask bag</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_odata_v4_curl(  # TODO -&gt; CURL command does not process url with ?$skip=100000 ath the end - returns same data as first link
    target_url: str,
):  # TODO -&gt; How to define Bag for type hinting? (https://docs.python.org/3/library/typing.html#newtype)
    &#34;&#34;&#34;Retrieves a table from a specific url for CBS Odata v4.

    Parameters
    ----------
    url_table_properties: str
        The url of the desired table

    Returns
    -------
    data: Dask bag
        All data received from target url as json type, in a Dask bag
    &#34;&#34;&#34;

    # First call target url and get json formatted response as dict
    temp_file = &#34;odata.json&#34;
    # r = requests.get(target_url).json()
    subprocess.run([&#34;curl&#34;, &#34;-fL&#34;, &#34;-o&#34;, temp_file, target_url])
    # Parse json file as dict
    with open(temp_file) as f:
        r = json.load(f)
    # Create Dask bag from dict
    bag = db.from_sequence(r[&#34;value&#34;])  # TODO -&gt; define npartitions?

    # check if more data exists
    if &#34;@odata.nextLink&#34; in r:
        target_url = r[&#34;@odata.nextLink&#34;]
    else:
        target_url = None

    # if more data exists continue to concat bag until complete
    while target_url:
        subprocess.run([&#34;curl&#34;, &#34;-fL&#34;, &#34;-o&#34;, temp_file, target_url])
        # Parse json file as dict
        with open(temp_file) as f:
            r = json.load(f)
        temp_bag = db.from_sequence(r[&#34;value&#34;])
        bag = db.concat([bag, temp_bag])

        if &#34;@odata.nextLink&#34; in r:
            target_url = r[&#34;@odata.nextLink&#34;]
        else:
            target_url = None

    return bag</code></pre>
</details>
</dd>
<dt id="statline_bq.utils.get_urls"><code class="name flex">
<span>def <span class="ident">get_urls</span></span>(<span>id: str, odata_version: str, third_party: bool = False)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns a dict with urls of all dataset tables given a valid CBS dataset id.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>id</code></strong> :&ensp;<code>str</code></dt>
<dd>CBS Dataset id, i.e. "83583NED"</dd>
<dt><strong><code>odata_version</code></strong> :&ensp;<code>str</code></dt>
<dd>version of the odata for this dataset - must be either "v3" or "v4".</dd>
<dt><strong><code>third_party</code></strong> :&ensp;<code>bool</code>, default=<code>False</code></dt>
<dd>Flag to indicate dataset is not originally from CBS. Set to true
to use dataderden.cbs.nl as base url (not available in v4 yet).</dd>
<dt>Returns:</dt>
<dt><strong><code>urls</code></strong> :&ensp;<code>dict</code> of <code>str</code></dt>
<dd>A dict containing all urls of a CBS dataset's tables</dd>
</dl>
<p>Examples:</p>
<pre><code class="language-python">&gt;&gt;&gt; dataset_id = '83583NED'
&gt;&gt;&gt; urls = get_urls(id=dataset_id, odata_version=&quot;v3&quot;, third_party=False)
&gt;&gt;&gt; for name, url in urls.items():
...     print(f&quot;{name}: {url}&quot;)
**```TableInfos```** :&amp;ensp;`&lt;https://opendata.cbs.nl/ODataFeed/odata/83583NED/TableInfos&gt;`
:   &amp;nbsp;


**```UntypedDataSet```** :&amp;ensp;`&lt;https://opendata.cbs.nl/ODataFeed/odata/83583NED/UntypedDataSet&gt;`
:   &amp;nbsp;


**```TypedDataSet```** :&amp;ensp;`&lt;https://opendata.cbs.nl/ODataFeed/odata/83583NED/TypedDataSet&gt;`
:   &amp;nbsp;


**```DataProperties```** :&amp;ensp;`&lt;https://opendata.cbs.nl/ODataFeed/odata/83583NED/DataProperties&gt;`
:   &amp;nbsp;


**```CategoryGroups```** :&amp;ensp;`&lt;https://opendata.cbs.nl/ODataFeed/odata/83583NED/CategoryGroups&gt;`
:   &amp;nbsp;


**```BedrijfstakkenBranchesSBI2008```** :&amp;ensp;`&lt;https://opendata.cbs.nl/ODataFeed/odata/83583NED/BedrijfstakkenBranchesSBI2008&gt;`
:   &amp;nbsp;


**```Bedrijfsgrootte```** :&amp;ensp;`&lt;https://opendata.cbs.nl/ODataFeed/odata/83583NED/Bedrijfsgrootte&gt;`
:   &amp;nbsp;


**```Perioden```** :&amp;ensp;`&lt;https://opendata.cbs.nl/ODataFeed/odata/83583NED/Perioden&gt;`
:   &amp;nbsp;


</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_urls(id: str, odata_version: str, third_party: bool = False):
    &#34;&#34;&#34;Returns a dict with urls of all dataset tables given a valid CBS dataset id.

    Parameters
    ----------
    id: str
        CBS Dataset id, i.e. &#34;83583NED&#34;

    odata_version: str
        version of the odata for this dataset - must be either &#34;v3&#34; or &#34;v4&#34;.

    third_party: bool, default=False
        Flag to indicate dataset is not originally from CBS. Set to true
        to use dataderden.cbs.nl as base url (not available in v4 yet).

    Returns:
    urls: dict of str
        A dict containing all urls of a CBS dataset&#39;s tables

    Examples:
    &gt;&gt;&gt; dataset_id = &#39;83583NED&#39;
    &gt;&gt;&gt; urls = get_urls(id=dataset_id, odata_version=&#34;v3&#34;, third_party=False)
    &gt;&gt;&gt; for name, url in urls.items():
    ...     print(f&#34;{name}: {url}&#34;)
    TableInfos: https://opendata.cbs.nl/ODataFeed/odata/83583NED/TableInfos
    UntypedDataSet: https://opendata.cbs.nl/ODataFeed/odata/83583NED/UntypedDataSet
    TypedDataSet: https://opendata.cbs.nl/ODataFeed/odata/83583NED/TypedDataSet
    DataProperties: https://opendata.cbs.nl/ODataFeed/odata/83583NED/DataProperties
    CategoryGroups: https://opendata.cbs.nl/ODataFeed/odata/83583NED/CategoryGroups
    BedrijfstakkenBranchesSBI2008: https://opendata.cbs.nl/ODataFeed/odata/83583NED/BedrijfstakkenBranchesSBI2008
    Bedrijfsgrootte: https://opendata.cbs.nl/ODataFeed/odata/83583NED/Bedrijfsgrootte
    Perioden: https://opendata.cbs.nl/ODataFeed/odata/83583NED/Perioden 
    &#34;&#34;&#34;

    if odata_version == &#34;v4&#34;:
        base_url = {
            True: None,  # currently no IV3 links in ODATA V4,
            False: f&#34;https://odata4.cbs.nl/CBS/{id}&#34;,
        }
        urls = {
            item[&#34;name&#34;]: base_url[third_party] + &#34;/&#34; + item[&#34;url&#34;]
            for item in get_odata_v4(base_url[third_party])
        }
    elif odata_version == &#34;v3&#34;:
        base_url = {
            True: f&#34;https://dataderden.cbs.nl/ODataFeed/odata/{id}?$format=json&#34;,
            False: f&#34;https://opendata.cbs.nl/ODataFeed/odata/{id}?$format=json&#34;,
        }
        urls = {
            item[&#34;name&#34;]: item[&#34;url&#34;]
            for item in requests.get(base_url[third_party]).json()[&#34;value&#34;]
        }
    else:
        raise ValueError(&#34;odata version must be either &#39;v3&#39; or &#39;v4&#39;&#34;)
    return urls</code></pre>
</details>
</dd>
<dt id="statline_bq.utils.main"><code class="name flex">
<span>def <span class="ident">main</span></span>(<span>id: str, source: str = 'cbs', third_party: bool = False, config: <a title="statline_bq.config.Config" href="config.html#statline_bq.config.Config">Config</a> = None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def main(
    id: str, source: str = &#34;cbs&#34;, third_party: bool = False, config: Config = None,
):
    print(f&#34;Processing dataset {id}&#34;)
    odata_version = check_v4(id=id, third_party=third_party)
    cbsodata_to_gbq(
        id=id,
        odata_version=odata_version,
        third_party=third_party,
        source=source,
        config=config,
    )
    print(
        f&#34;Completed dataset {id}&#34;
    )  # TODO - add response from google if possible (some success/failure flag)
    return None</code></pre>
</details>
</dd>
<dt id="statline_bq.utils.tables_to_parquet"><code class="name flex">
<span>def <span class="ident">tables_to_parquet</span></span>(<span>id: str, urls: dict, odata_version: str, source: str = 'cbs', pq_dir: Union[pathlib.Path, str] = None) ‑> set</span>
</code></dt>
<dd>
<div class="desc"><p>Downloads all tables related to a valid CBS dataset id, and stores them locally as Parquet files.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>id</code></strong> :&ensp;<code>str</code></dt>
<dd>CBS Dataset id, i.e. "83583NED"</dd>
<dt><strong><code>urls</code></strong> :&ensp;<code>dict</code></dt>
<dd>Dictionary holding urls of all dataset tables from CBS</dd>
<dt><strong><code>odata_version</code></strong> :&ensp;<code>str</code></dt>
<dd>version of the odata for this dataset - must be either "v3" or "v4".</dd>
<dt><strong><code>source</code></strong> :&ensp;<code>str</code>, default=<code>'cbs</code></dt>
<dd>The source of the dataset. Currently only "cbs" is relevant.</dd>
<dt><strong><code>pq_dir</code></strong> :&ensp;<code>Path</code> or <code>str</code></dt>
<dd>The directory where the putput Parquet files are stored.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>files_parquet</code></strong> :&ensp;<code>set</code> of <code>Path</code></dt>
<dd>A set containing Path objects of all output Parquet files</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def tables_to_parquet(
    id: str,
    urls: dict,
    odata_version: str,
    source: str = &#34;cbs&#34;,
    pq_dir: Union[Path, str] = None,
) -&gt; set:
    &#34;&#34;&#34;Downloads all tables related to a valid CBS dataset id, and stores them locally as Parquet files.

    Parameters
    ----------
    id : str
        CBS Dataset id, i.e. &#34;83583NED&#34;
    urls : dict
        Dictionary holding urls of all dataset tables from CBS
    odata_version : str
        version of the odata for this dataset - must be either &#34;v3&#34; or &#34;v4&#34;.
    source : str, default=&#39;cbs
        The source of the dataset. Currently only &#34;cbs&#34; is relevant.
    pq_dir : Path or str
        The directory where the putput Parquet files are stored.

    Returns
    -------
    files_parquet: set of Path
        A set containing Path objects of all output Parquet files
    &#34;&#34;&#34;

    # Create placeholders for storage
    files_parquet = set()

    # Iterate over all tables related to dataset, excepet Properties (from v4), TableInfos (from v3) and UntypedDataset (from v3) (TODO -&gt; double check that it is redundandt)
    for key, url in [
        (k, v)
        for k, v in urls.items()
        if k
        not in (
            &#34;Properties&#34;,
            &#34;TableInfos&#34;,
            &#34;UntypedDataSet&#34;,
        )  # Redundant tables from v3 AND v4
    ]:

        # for v3 urls an appendix of &#34;?format=json&#34; is needed
        if odata_version == &#34;v3&#34;:
            url = &#34;?&#34;.join((url, &#34;$format=json&#34;))

        # Create table name to be used in GCS
        table_name = f&#34;{source}.{odata_version}.{id}_{key}&#34;

        # Get data from source
        table = get_odata(target_url=url, odata_version=odata_version)

        # Check if get_odata returned None (when link in CBS returns empty table, i.e. CategoryGroups in &#34;84799NED&#34; - seems only relevant for v3 only)
        if table is not None:

            # Convert to parquet
            pq_path = convert_table_to_parquet(table, table_name, pq_dir)

            # Add path of file to set
            files_parquet.add(pq_path)

    return files_parquet</code></pre>
</details>
</dd>
<dt id="statline_bq.utils.upload_to_gcs"><code class="name flex">
<span>def <span class="ident">upload_to_gcs</span></span>(<span>dir: pathlib.Path, source: str = 'cbs', odata_version: str = None, id: str = None, config: <a title="statline_bq.config.Config" href="config.html#statline_bq.config.Config">Config</a> = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Uploads all files in a given directory to Google Cloud Storage.</p>
<p>This function is meant to be used for uploading all tables of a certain dataset retrieved from
the CBS API. It therefore uses the following naming structure as the GCS blobs:</p>
<pre><code>"{project_name}/{bucket_name}/{source}/{odata_version}/{id}/{YYYYMMDD}/{filename}"
</code></pre>
<p>For example, dataset "82807NED", uploaded on Novemeber 11, 2020, to the "dataverbinders" project,
using "dataverbinders" as a bucket, would create the following:</p>
<ul>
<li>"dataverbinders/dataverbinders/cbs/v4/83765NED/20201104/cbs.82807NED_Observations.parquet"</li>
<li>"dataverbinders/dataverbinders/cbs/v4/83765NED/20201104/cbs.82807NED_PeriodenCodes.parquet"</li>
<li>etc..</li>
</ul>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dir</code></strong> :&ensp;<code>Path</code></dt>
<dd>A Path object to a directory containing files to be uploaded</dd>
<dt><strong><code>source</code></strong> :&ensp;<code>str</code>, default=<code>"cbs"</code></dt>
<dd>The source of the dataset. Currently only "cbs" is relevant.</dd>
<dt><strong><code>odata_version</code></strong> :&ensp;<code>str</code></dt>
<dd>version of the odata for this dataset - must be either "v3" or "v4".</dd>
<dt><strong><code>id</code></strong> :&ensp;<code>str</code></dt>
<dd>CBS Dataset id, i.e. "83583NED"</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>gcs_folder</code></strong> :&ensp;<code>str</code></dt>
<dd>The folder (=blob) into which the tables have been uploaded # TODO -&gt; Return success/ fail code?/job ID</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def upload_to_gcs(
    dir: Path,
    source: str = &#34;cbs&#34;,
    odata_version: str = None,
    id: str = None,
    config: Config = None,
):
    &#34;&#34;&#34;Uploads all files in a given directory to Google Cloud Storage.

    This function is meant to be used for uploading all tables of a certain dataset retrieved from
    the CBS API. It therefore uses the following naming structure as the GCS blobs:

        &#34;{project_name}/{bucket_name}/{source}/{odata_version}/{id}/{YYYYMMDD}/{filename}&#34;

    For example, dataset &#34;82807NED&#34;, uploaded on Novemeber 11, 2020, to the &#34;dataverbinders&#34; project,
    using &#34;dataverbinders&#34; as a bucket, would create the following:

    - &#34;dataverbinders/dataverbinders/cbs/v4/83765NED/20201104/cbs.82807NED_Observations.parquet&#34;
    - &#34;dataverbinders/dataverbinders/cbs/v4/83765NED/20201104/cbs.82807NED_PeriodenCodes.parquet&#34;
    - etc..

    Parameters
    ----------
    dir: Path
        A Path object to a directory containing files to be uploaded
    source: str, default=&#34;cbs&#34;
        The source of the dataset. Currently only &#34;cbs&#34; is relevant.
    odata_version: str
        version of the odata for this dataset - must be either &#34;v3&#34; or &#34;v4&#34;.
    id: str
        CBS Dataset id, i.e. &#34;83583NED&#34;

    Returns
    -------
    gcs_folder: str
        The folder (=blob) into which the tables have been uploaded # TODO -&gt; Return success/ fail code?/job ID
    &#34;&#34;&#34;

    # Initialize Google Storage Client, get bucket, set blob
    gcs = storage.Client(
        project=config.gcp.dev.project_id
    )  # TODO -&gt; handle dev, test and prod appropriatley
    gcs_bucket = gcs.get_bucket(config.gcp.dev.bucket)
    gcs_folder = (
        f&#34;{source}/{odata_version}/{id}/{datetime.today().date().strftime(&#39;%Y%m%d&#39;)}&#34;
    )
    # Upload file
    for pfile in listdir(dir):
        gcs_blob = gcs_bucket.blob(gcs_folder + &#34;/&#34; + pfile)
        gcs_blob.upload_from_filename(
            dir / pfile
        )  # TODO: job currently returns None. Also how to handle if we get it?

    return gcs_folder  # TODO: return job id, if possible</code></pre>
</details>
</dd>
<dt id="statline_bq.utils.write_description_to_file"><code class="name flex">
<span>def <span class="ident">write_description_to_file</span></span>(<span>id: str, description_text: str, pq_dir: Union[pathlib.Path, str], source: str = 'cbs', odata_version: str = None) ‑> pathlib.Path</span>
</code></dt>
<dd>
<div class="desc"><p>Writes a string into a text file at a given location.</p>
<p>Writes a dataset description string into a txt file and places that
file in a directory alongside the rest of that dataset's tables (assuming
it, and they exist). The file is named according to the same conventions
used for the tables, and placed in the directory accordingly, namely:</p>
<pre><code>"{source}.{odata_version}.{id}_Description.txt"
</code></pre>
<p>for example:</p>
<pre><code>"cbs.v3.83583NED_Description.txt"
</code></pre>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>id</code></strong> :&ensp;<code>str</code></dt>
<dd>CBS Dataset id, i.e. "83583NED"</dd>
<dt><strong><code>description_text</code></strong> :&ensp;<code>str</code></dt>
<dd>The dataset description text to be written into the file.</dd>
<dt><strong><code>pq_dir</code></strong> :&ensp;<code>Path</code> or <code>str</code></dt>
<dd>Path to directory where the file will be stored.</dd>
<dt><strong><code>source</code></strong> :&ensp;<code>str</code>, default=<code>"cbs"</code></dt>
<dd>The source of the dataset. Currently only "cbs" is relevant.</dd>
<dt><strong><code>odata_version</code></strong> :&ensp;<code>str</code></dt>
<dd>The version of the OData for this dataset - should be "v3" or "v4".</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>description_file</code></strong> :&ensp;<code>Path</code></dt>
<dd>A path of the output txt file</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def write_description_to_file(
    id: str,
    description_text: str,
    pq_dir: Union[Path, str],
    source: str = &#34;cbs&#34;,
    odata_version: str = None,
) -&gt; Path:
    &#34;&#34;&#34;Writes a string into a text file at a given location.

    Writes a dataset description string into a txt file and places that
    file in a directory alongside the rest of that dataset&#39;s tables (assuming
    it, and they exist). The file is named according to the same conventions
    used for the tables, and placed in the directory accordingly, namely:

        &#34;{source}.{odata_version}.{id}_Description.txt&#34;

    for example:

        &#34;cbs.v3.83583NED_Description.txt&#34;

    Parameters
    ----------
    id: str
        CBS Dataset id, i.e. &#34;83583NED&#34;
    description_text: str
        The dataset description text to be written into the file.
    pq_dir: Path or str
        Path to directory where the file will be stored.
    source: str, default=&#34;cbs&#34;
        The source of the dataset. Currently only &#34;cbs&#34; is relevant.
    odata_version: str
        The version of the OData for this dataset - should be &#34;v3&#34; or &#34;v4&#34;.

    Returns
    -------
    description_file: Path
        A path of the output txt file
    &#34;&#34;&#34;

    description_file = Path(pq_dir) / Path(
        f&#34;{source}.{odata_version}.{id}_Description.txt&#34;
    )
    with open(description_file, &#34;w+&#34;) as f:
        f.write(description_text)
    return description_file</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="statline_bq" href="index.html">statline_bq</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="statline_bq.utils.cbsodata_to_gbq" href="#statline_bq.utils.cbsodata_to_gbq">cbsodata_to_gbq</a></code></li>
<li><code><a title="statline_bq.utils.check_bq_dataset" href="#statline_bq.utils.check_bq_dataset">check_bq_dataset</a></code></li>
<li><code><a title="statline_bq.utils.check_v4" href="#statline_bq.utils.check_v4">check_v4</a></code></li>
<li><code><a title="statline_bq.utils.convert_table_to_parquet" href="#statline_bq.utils.convert_table_to_parquet">convert_table_to_parquet</a></code></li>
<li><code><a title="statline_bq.utils.create_bq_dataset" href="#statline_bq.utils.create_bq_dataset">create_bq_dataset</a></code></li>
<li><code><a title="statline_bq.utils.create_dir" href="#statline_bq.utils.create_dir">create_dir</a></code></li>
<li><code><a title="statline_bq.utils.create_named_dir" href="#statline_bq.utils.create_named_dir">create_named_dir</a></code></li>
<li><code><a title="statline_bq.utils.delete_bq_dataset" href="#statline_bq.utils.delete_bq_dataset">delete_bq_dataset</a></code></li>
<li><code><a title="statline_bq.utils.gcs_to_gbq" href="#statline_bq.utils.gcs_to_gbq">gcs_to_gbq</a></code></li>
<li><code><a title="statline_bq.utils.get_dataset_description" href="#statline_bq.utils.get_dataset_description">get_dataset_description</a></code></li>
<li><code><a title="statline_bq.utils.get_dataset_description_v3" href="#statline_bq.utils.get_dataset_description_v3">get_dataset_description_v3</a></code></li>
<li><code><a title="statline_bq.utils.get_dataset_description_v4" href="#statline_bq.utils.get_dataset_description_v4">get_dataset_description_v4</a></code></li>
<li><code><a title="statline_bq.utils.get_description_from_gcs" href="#statline_bq.utils.get_description_from_gcs">get_description_from_gcs</a></code></li>
<li><code><a title="statline_bq.utils.get_file_names" href="#statline_bq.utils.get_file_names">get_file_names</a></code></li>
<li><code><a title="statline_bq.utils.get_odata" href="#statline_bq.utils.get_odata">get_odata</a></code></li>
<li><code><a title="statline_bq.utils.get_odata_v3" href="#statline_bq.utils.get_odata_v3">get_odata_v3</a></code></li>
<li><code><a title="statline_bq.utils.get_odata_v4" href="#statline_bq.utils.get_odata_v4">get_odata_v4</a></code></li>
<li><code><a title="statline_bq.utils.get_odata_v4_curl" href="#statline_bq.utils.get_odata_v4_curl">get_odata_v4_curl</a></code></li>
<li><code><a title="statline_bq.utils.get_urls" href="#statline_bq.utils.get_urls">get_urls</a></code></li>
<li><code><a title="statline_bq.utils.main" href="#statline_bq.utils.main">main</a></code></li>
<li><code><a title="statline_bq.utils.tables_to_parquet" href="#statline_bq.utils.tables_to_parquet">tables_to_parquet</a></code></li>
<li><code><a title="statline_bq.utils.upload_to_gcs" href="#statline_bq.utils.upload_to_gcs">upload_to_gcs</a></code></li>
<li><code><a title="statline_bq.utils.write_description_to_file" href="#statline_bq.utils.write_description_to_file">write_description_to_file</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.8.5</a>.</p>
</footer>
</body>
</html>